{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def wavPlayer(filepath):\n",
    "    \"\"\" will display html 5 player for compatible browser\n",
    "\n",
    "    Parameters :\n",
    "    ------------\n",
    "    filepath : relative filepath with respect to the notebook directory ( where the .ipynb are not cwd)\n",
    "               of the file to play\n",
    "\n",
    "    The browser need to know how to play wav through html5.\n",
    "\n",
    "    there is no autoplay to prevent file playing when the browser opens\n",
    "    \"\"\"\n",
    "    \n",
    "    src = \"\"\"\n",
    "    <head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
    "    <title>Simple Test</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "    <audio controls=\"controls\" style=\"width:600px\" >\n",
    "      <source src=\"files/%s\" type=\"audio/wav\" />\n",
    "      Your browser does not support the audio element.\n",
    "    </audio>\n",
    "    </body>\n",
    "    \"\"\"%(filepath)\n",
    "    display(HTML(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "s_dict = {'table_start': 0, 'hand_start': 1, 'off_start': 2}\n",
    "fi_dict = {'nothing': 0, 'pasta': 1, 'rice': 2, 'water': 3}\n",
    "fu_dict = {'zero': 0, 'fifty': 1, 'ninety': 2}\n",
    "b_dict = {'regular': 0, 'textured': 1}\n",
    "l_dict = {'light0': 0, 'light1': 1}\n",
    "c_dict = {'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4}\n",
    "obj_id_dict = {1: 'red cup', 2: 'small white cup',3:'small transparent cup',4:'green glass',5:'wine glass',\n",
    "              6:'champagne flute glass', 7:'cereal box',8:'biscuit box',9:'tea box', 10: 'sth', 11: 'sth2', 12: 'sth3'} \n",
    "\n",
    "valid_dict = {'s': list(s_dict.keys()), \n",
    "              'fi': list(fi_dict.keys()),\n",
    "              'fu': list(fu_dict.keys()),\n",
    "              'b': list(b_dict.keys()),\n",
    "              'l': list(l_dict.keys()),\n",
    "              'c': list(c_dict.keys()),\n",
    "              'obj_id': list(obj_id_dict.keys()),\n",
    "             }\n",
    "\n",
    "def retrieve_data(obj_id, s, fi, fu, b, l, c=[]):\n",
    "    if ((fi == 'nothing' and (fu =='fifty' or fu =='ninety')) or (fi == 'pasta' and fu == 'zero') or (fi == 'rice' and fu=='zero') or (fi=='water' and fu=='zero')): \n",
    "        #print('error')\n",
    "        return -1\n",
    "    for i in range(1,len(c),1):\n",
    "        if c[i] not in valid_dict['c']:\n",
    "            return -1\n",
    "    if  (obj_id not in obj_id_dict) or (s not in valid_dict['s']) or (fi not in valid_dict['fi']) or (fu not in valid_dict['fu']) or (b not in valid_dict['b']) or (l not in valid_dict['l']) :\n",
    "        return -1\n",
    "    \n",
    "    _obj_id = obj_id\n",
    "    _s_id = s_dict[s]\n",
    "    _fi_id = fi_dict[fi]\n",
    "    _fu_id = fu_dict[fu]\n",
    "    _b_id = b_dict[b]\n",
    "    _l_id = l_dict[l]\n",
    "    _c_id = []\n",
    "    \n",
    "    for i in range(0,len(c),1):\n",
    "        _c_id.append(c_dict[c[i]])\n",
    "    if(len(c)==0):\n",
    "        _c_id = [1,2,3,4]\n",
    "        \n",
    "    input_string = 's'+str(_s_id)+'_fi'+str(_fi_id)+'_fu'+str(_fu_id)+'_b'+str(_b_id)+'_l'+str(_l_id)\n",
    "    \n",
    "    audio_path = \"./*Dataset/\"+str(_obj_id)+\"/audio/\"+input_string+\"*\"\n",
    "    audio_list = glob.glob(audio_path)[0]\n",
    "    \n",
    "    calib_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        calib_path = \"./*Dataset/\"+str(_obj_id)+\"/calib/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        calib_list.append(glob.glob(calib_path)[0])\n",
    "    \n",
    "    depth_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        depth_path = \"./*Dataset/\"+str(_obj_id)+\"/depth/\"+input_string+'/c'+str(_c_id[i])+'/*'\n",
    "        depth_list.append(glob.glob(depth_path))\n",
    "        \n",
    "    imu_path = \"./*Dataset/\"+str(_obj_id)+\"/imu/\"+input_string+\"*\"\n",
    "    imu_list = tuple(glob.glob(imu_path))\n",
    "    \n",
    "    \n",
    "    ir_list=[]\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        ir_path = \"./*Dataset/\"+str(_obj_id)+\"/ir/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        ir_list.append(glob.glob(ir_path))\n",
    "    \n",
    "    \n",
    "    rgb_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        rgb_path = \"./*Dataset/\"+str(_obj_id)+\"/rgb/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        rgb_list.append(glob.glob(rgb_path)[0])\n",
    "    \n",
    "    \n",
    "    output_dict = {'audio': audio_list,'calib':calib_list,'depth':depth_list,'imu':imu_list,'ir':ir_list,'rgb':rgb_list}\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "N_MFCC = 40\n",
    "\n",
    "def load_and_extract_mfcc(path, window_size=20.0/1000, max_length=30):\n",
    "    # Load audio data from file\n",
    "    audio, sample_rate = librosa.load(path, res_type='kaiser_fast')\n",
    "    print(audio)\n",
    "    print(sample_rate)\n",
    "    \n",
    "    length = audio.shape[0]\n",
    "    step_size = int(window_size * sample_rate) # In samples    \n",
    "    \n",
    "    # Retrieve the sequence of MFCCs\n",
    "    sequence = librosa.feature.mfcc(y=audio[:max_length*sample_rate], sr=sample_rate, n_mfcc=N_MFCC,\n",
    "                                    hop_length=step_size)\n",
    "    sequence = np.transpose(sequence)\n",
    "    \n",
    "    # Normalize the sequence according to its own data\n",
    "    ### Normalization for each MFCC individually\n",
    "    _mean = np.mean(sequence, axis=0)\n",
    "    _std = np.std(sequence, axis=0)\n",
    "    \n",
    "    return (sequence - _mean) / _std\n",
    "\n",
    "aux_dict = {'nothing': np.array([1., 0., 0., 0.]),\n",
    "            'pasta': np.array([0., 1., 0., 0.]),\n",
    "            'rice': np.array([0., 0., 1., 0.]),\n",
    "            'water': np.array([0., 0., 0., 1.])}\n",
    "data = []\n",
    "labels = []\n",
    "#LONGEST_SEQUENCE = 0\n",
    "for obj_id in range(1, 10):\n",
    "    print(f\"Extracting data from object id: `{obj_id}`\")\n",
    "    for sit in s_dict.keys():\n",
    "        for fi in fi_dict.keys():\n",
    "            for fu in fu_dict.keys():\n",
    "                for b in b_dict.keys():\n",
    "                    for l in l_dict.keys():\n",
    "                        try:\n",
    "                            sample = retrieve_data(obj_id, s=sit, fi=fi, fu=fu, b=b, l=l)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed...: {(obj_id, sit, fi, fu, b, l)}\")\n",
    "                        if sample != -1:\n",
    "                            seq_data = load_and_extract_mfcc(sample['audio'])\n",
    "                            data.append(torch.Tensor(seq_data))\n",
    "                            labels.append(fi_dict[fi])\n",
    "\n",
    "data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "#labels = torch.LongTensor(labels)\n",
    "N_SAMPLES = len(data)\n",
    "print(f\"Got {N_SAMPLES} samples in total.\")\n",
    "print(len(data), len(labels))\n",
    "print(data.shape)\n",
    "SEQUENCE_LENGTH = data.shape[1]\n",
    "SEQUENCE_FEATURES = data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Analyze class distribution and \n",
    "_classes, counts = np.unique(labels, return_counts=True)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Whole dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                   test_size=0.1)\n",
    "\n",
    "_classes, counts = np.unique(y_train, return_counts=True)\n",
    "n_train_samples = len(y_train)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Train dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Compute the class weights used for training\n",
    "CLASS_WEIGHTS = np.array([c / n_train_samples for c in counts])\n",
    "CLASS_WEIGHTS = 1.0 / CLASS_WEIGHTS\n",
    "CLASS_WEIGHTS = CLASS_WEIGHTS / CLASS_WEIGHTS.sum()\n",
    "print(CLASS_WEIGHTS)\n",
    "\n",
    "_classes, counts = np.unique(y_test, return_counts=True)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Test dataset\")\n",
    "plt.show()\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(self.x) == len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.x[idx]), self.y[idx]\n",
    "    \n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
    "                          num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = list(fi_dict.keys())\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=(120, 40),\n",
    "                               stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16,\n",
    "                               out_channels=8,\n",
    "                               kernel_size=(40, 1),\n",
    "                               stride=2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.linear1 = nn.Linear(8*154, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o1 = F.relu(self.conv1(x))\n",
    "        o1 = self.dropout1(o1)\n",
    "        #print(o1.shape)\n",
    "        o2 = F.relu(self.conv2(o1))\n",
    "        o2 = self.dropout2(o2)\n",
    "        #print(o2.shape)\n",
    "        \n",
    "        out = self.linear1(o2.view(-1, 8*154))\n",
    "        \n",
    "        return out\n",
    "\n",
    "INTERVAL = 35\n",
    "    \n",
    "model = ConvNet()\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00025, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(CLASS_WEIGHTS).cuda())\n",
    "\n",
    "for epoch in range(500):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        x, y = batch[0].cuda(), batch[1].cuda()\n",
    "        x = x.unsqueeze(1) # For conv net\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_y = model(x)\n",
    "        \n",
    "        loss = criterion(pred_y, y)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % INTERVAL == INTERVAL-1:\n",
    "            running_loss += running_loss\n",
    "            print(f\"[{epoch+1}, {i_batch+1}]: Loss: {running_loss/INTERVAL:.4f}\")\n",
    "            \n",
    "    if epoch % 10 == 9:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in test_loader:\n",
    "                x, y = batch[0].cuda(), batch[1].cuda()\n",
    "                x = x.unsqueeze(1)\n",
    "                \n",
    "                y_true += list(y.cpu().numpy())\n",
    "                \n",
    "                pred_y = model(x)\n",
    "                _, predicted = torch.max(pred_y.data, 1)\n",
    "                y_pred += list(predicted.cpu().numpy())\n",
    "                \n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                \n",
    "        print(len(y_true), len(y_pred))\n",
    "        print(y_true[0])\n",
    "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "                \n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Test Acc: {acc:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Dataset/10/audio/0012_audio.wav'\n",
    "wavPlayer(path)\n",
    "\n",
    "#fi_dict = {'nothing': 0, 'pasta': 1, 'rice': 2, 'water': 3}\n",
    "rev = {0: 'nothing', 1: 'pasta', 2: 'rice', 3: 'water'}\n",
    "\n",
    "def getModelPrediction(model, path):\n",
    "    # Loading audio file and extracting MFCC feature\n",
    "    features = load_and_extract_mfcc(path)\n",
    "    \n",
    "    # Padding the sequence\n",
    "    pad_length = SEQUENCE_LENGTH - features.shape[0]\n",
    "    features = np.concatenate([features, np.zeros((pad_length, N_MFCC))])\n",
    "\n",
    "    feat_tensor = torch.Tensor(features)\n",
    "    feat_tensor = feat_tensor.unsqueeze(0)\n",
    "    feat_tensor = feat_tensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(feat_tensor.cuda())\n",
    "        scores = F.softmax(out, dim=1).cpu().numpy()\n",
    "\n",
    "        _class = np.argmax(scores)\n",
    "        score = np.max(scores)\n",
    "\n",
    "        print(f\"`{rev[_class]}` with {score*100}% confidence\")\n",
    "\n",
    "        print(np.round(scores, 2))\n",
    "        \n",
    "    return _class\n",
    "getModelPrediction(model,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from natsort import natsorted \n",
    "prediction_list = []\n",
    "for i in range(10,13,1):\n",
    "    path = './Dataset/'+str(i)+'/audio/*'\n",
    "    test_list = natsorted(glob.glob(path))\n",
    "    for tl in test_list:\n",
    "        print(tl)\n",
    "    print(\"#############\")\n",
    "    #print(test_list)\n",
    "    for i in range(0,len(test_list),1):\n",
    "        prediction_list.append(getModelPrediction(model,test_list[i]))\n",
    "print(len(prediction_list))        \n",
    "print(prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatesubmission(feature,data_list):\n",
    "    df = pd.read_csv('submissions/submissionfile.csv',index_col=0)\n",
    "    df[feature] = data_list\n",
    "    df.to_csv('./submissions/submissionfile-record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatesubmission('Filling type',prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
