{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "s_dict = {'table_start': 0, 'hand_start': 1, 'off_start': 2}\n",
    "fi_dict = {'nothing': 0, 'pasta': 1, 'rice': 2, 'water': 3}\n",
    "fu_dict = {'zero': 0, 'fifty': 1, 'ninety': 2}\n",
    "b_dict = {'regular': 0, 'textured': 1}\n",
    "l_dict = {'light0': 0, 'light1': 1}\n",
    "c_dict = {'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4}\n",
    "obj_id_dict = {1: 'red cup', 2: 'small white cup',3:'small transparent cup',4:'green glass',5:'wine glass',\n",
    "              6:'champagne flute glass', 7:'cereal box',8:'biscuit box',9:'tea box'} \n",
    "\n",
    "valid_dict = {'s': list(s_dict.keys()), \n",
    "              'fi': list(fi_dict.keys()),\n",
    "              'fu': list(fu_dict.keys()),\n",
    "              'b': list(b_dict.keys()),\n",
    "              'l': list(l_dict.keys()),\n",
    "              'c': list(c_dict.keys()),\n",
    "              'obj_id': list(obj_id_dict.keys()),\n",
    "             }\n",
    "\n",
    "def retrieve_data(obj_id, s, fi, fu, b, l, c=[]):\n",
    "    if ((fi == 'nothing' and (fu =='fifty' or fu =='ninety')) or (fi == 'pasta' and fu == 'zero') or (fi == 'rice' and fu=='zero') or (fi=='water' and fu=='zero')): \n",
    "        #print('error')\n",
    "        return -1\n",
    "    for i in range(1,len(c),1):\n",
    "        if c[i] not in valid_dict['c']:\n",
    "            return -1\n",
    "    if  (obj_id not in obj_id_dict) or (s not in valid_dict['s']) or (fi not in valid_dict['fi']) or (fu not in valid_dict['fu']) or (b not in valid_dict['b']) or (l not in valid_dict['l']) :\n",
    "        return -1\n",
    "    \n",
    "    _obj_id = obj_id\n",
    "    _s_id = s_dict[s]\n",
    "    _fi_id = fi_dict[fi]\n",
    "    _fu_id = fu_dict[fu]\n",
    "    _b_id = b_dict[b]\n",
    "    _l_id = l_dict[l]\n",
    "    _c_id = []\n",
    "    \n",
    "    for i in range(0,len(c),1):\n",
    "        _c_id.append(c_dict[c[i]])\n",
    "    if(len(c)==0):\n",
    "        _c_id = [1,2,3,4]\n",
    "        \n",
    "    input_string = 's'+str(_s_id)+'_fi'+str(_fi_id)+'_fu'+str(_fu_id)+'_b'+str(_b_id)+'_l'+str(_l_id)\n",
    "    \n",
    "    audio_path = \"./*Dataset/\"+str(_obj_id)+\"/audio/\"+input_string+\"*\"\n",
    "    audio_list = glob.glob(audio_path)[0]\n",
    "    \n",
    "    calib_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        calib_path = \"./*Dataset/\"+str(_obj_id)+\"/calib/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        calib_list.append(glob.glob(calib_path)[0])\n",
    "    \n",
    "    depth_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        depth_path = \"./*Dataset/\"+str(_obj_id)+\"/depth/\"+input_string+'/c'+str(_c_id[i])+'/*'\n",
    "        depth_list.append(natsorted(glob.glob(depth_path), key=lambda y: y.lower()))\n",
    "        \n",
    "    imu_path = \"./*Dataset/\"+str(_obj_id)+\"/imu/\"+input_string+\"*\"\n",
    "    imu_list = tuple(glob.glob(imu_path))\n",
    "    \n",
    "    \n",
    "    ir_list=[]\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        ir_path = \"./*Dataset/\"+str(_obj_id)+\"/ir/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        ir_list.append(glob.glob(ir_path))\n",
    "    \n",
    "    \n",
    "    rgb_list = []\n",
    "    for i in range(0,len(_c_id),1):\n",
    "        rgb_path = \"./*Dataset/\"+str(_obj_id)+\"/rgb/\"+input_string+'_c'+str(_c_id[i])+'*'\n",
    "        rgb_list.append(glob.glob(rgb_path)[0])\n",
    "    \n",
    "    \n",
    "    output_dict = {'audio': audio_list,'calib':calib_list,'depth':depth_list,'imu':imu_list,'ir':ir_list,'rgb':rgb_list}\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from object id: `1`\n",
      "Extracting data from object id: `2`\n",
      "Extracting data from object id: `3`\n",
      "Failed for ./Dataset/3/depth/s0_fi3_fu2_b0_l1/c3/0307.png\n",
      "Extracting data from object id: `4`\n",
      "Extracting data from object id: `5`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-58dd8f75e5c1>:35: RuntimeWarning: invalid value encountered in true_divide\n",
      "  filter1_8u = ((filter1 / filter1.max()) * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for ./Dataset/5/depth/s2_fi1_fu1_b0_l0/c3/0206.png\n",
      "Failed for ./Dataset/5/depth/s2_fi1_fu2_b0_l0/c3/0335.png\n",
      "Extracting data from object id: `6`\n",
      "Extracting data from object id: `7`\n",
      "Failed for ./Dataset/7/depth/s2_fi0_fu0_b1_l0/c3/0098.png\n",
      "Failed for ./Dataset/7/depth/s2_fi1_fu1_b1_l0/c3/0100.png\n",
      "Failed for ./Dataset/7/depth/s2_fi1_fu2_b1_l0/c3/0105.png\n",
      "Failed for ./Dataset/7/depth/s2_fi2_fu1_b0_l0/c3/0139.png\n",
      "Extracting data from object id: `8`\n",
      "Failed for ./Dataset/8/depth/s2_fi1_fu1_b0_l0/c3/0140.png\n",
      "Failed for ./Dataset/8/depth/s2_fi1_fu2_b0_l0/c3/0143.png\n",
      "Failed for ./Dataset/8/depth/s2_fi2_fu1_b0_l0/c3/0134.png\n",
      "Extracting data from object id: `9`\n",
      "Failed for ./Dataset/9/depth/s2_fi1_fu1_b0_l0/c3/0132.png\n",
      "Failed for ./Dataset/9/depth/s2_fi2_fu2_b0_l0/c3/0131.png\n",
      "672 672\n",
      "Out of 684 samples, 12 failed to find object ROI\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def computeObjectBoundingBox(depth_img_path, debug=False):\n",
    "    # Define height limit to search, since bottom area is always a table\n",
    "    HEIGHT_LIMIT = 600\n",
    "    # Minimum area of the contours to be considered\n",
    "    MIN_CONTOUR_AREA = 15000\n",
    "    # In percentage (of bounding box)\n",
    "    BBOX_EXPANSION = 1.05\n",
    "\n",
    "    # Load image as uint16\n",
    "    cv_img = cv2.imread(depth_img_path, -1)[:HEIGHT_LIMIT]\n",
    "\n",
    "    if debug:\n",
    "        # Show image\n",
    "        plt.imshow(cv_img)\n",
    "        plt.title(\"Original Depth Image\")\n",
    "        plt.show()\n",
    "\n",
    "    # Find stats of image\n",
    "    #mean = np.mean(cv_img)\n",
    "    #std = np.std(cv_img)\n",
    "\n",
    "    # Define distance threshold\n",
    "    DIST_THRESHOLD = 700\n",
    "    #DIST_THRESHOLD = mean - std\n",
    "\n",
    "    # Filter pixels by distance threshold\n",
    "    filter1 = np.where(cv_img < DIST_THRESHOLD, cv_img, 0)\n",
    "\n",
    "    # Extract contours\n",
    "    # Convert to unsigned 8-bit\n",
    "    filter1_8u = ((filter1 / filter1.max()) * 255).astype(np.uint8)\n",
    "    # Apply closing operation, try to retrieve some of the \"missing\" regions\n",
    "    kernel = np.ones((15, 15), np.uint8)\n",
    "    filter1_8u = cv2.morphologyEx(filter1_8u, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(filter1_8u, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        if debug:\n",
    "            print(\"Couldn't find any contours\")\n",
    "            \n",
    "        return cv_img, None\n",
    "\n",
    "    if debug:\n",
    "        viz_img = np.dstack([filter1_8u.copy(), filter1_8u.copy(), filter1_8u.copy()])\n",
    "\n",
    "    # Iterate over contours, find the largest one\n",
    "    #candidates = []\n",
    "    bestContour = None\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > MIN_CONTOUR_AREA and bestContour is None:\n",
    "            bestContour = contour\n",
    "        elif area > MIN_CONTOUR_AREA and bestContour is not None:\n",
    "            # Use contour to find mask\n",
    "            curr_cnt_mask = np.zeros(cv_img.shape, np.uint8)\n",
    "            cv2.drawContours(curr_cnt_mask, [contour], 0, 255, -1)\n",
    "            \n",
    "            best_cnt_mask = np.zeros(cv_img.shape, np.uint8)\n",
    "            cv2.drawContours(best_cnt_mask, [bestContour], 0, 255, -1)\n",
    "            \n",
    "            best_cnt_mean = cv2.mean(cv_img, mask=best_cnt_mask)\n",
    "            curr_cnt_mean = cv2.mean(cv_img, mask=curr_cnt_mask)\n",
    "            \n",
    "            if curr_cnt_mean[0] < best_cnt_mean[0]:\n",
    "                bestContour = contour\n",
    "            \n",
    "            if debug:\n",
    "                plt.imshow(curr_cnt_mask, cmap='gray')\n",
    "                plt.title(\"Current contour mask\")\n",
    "                plt.show()\n",
    "                plt.imshow(best_cnt_mask, cmap='gray')\n",
    "                plt.title(\"best contour mask\")\n",
    "                plt.show()            \n",
    "\n",
    "    # Given the largest contour, find the bounding box\n",
    "    if bestContour is not None:\n",
    "        #print(\"Found object contour!\")\n",
    "\n",
    "        # If we found our object contour, find the bounding box\n",
    "        x, y , w, h = cv2.boundingRect(bestContour)\n",
    "\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"Couldn't determine object contour\")\n",
    "        return cv_img, None\n",
    "    \n",
    "    # Expand the bounding box a bit\n",
    "    inc_w_h = int((w * BBOX_EXPANSION - w) / 2)\n",
    "    inc_h_h = int((h * BBOX_EXPANSION - h) / 2)\n",
    "    \n",
    "    topx = x - inc_w_h\n",
    "    topy = y - inc_h_h\n",
    "    botx = x + w + inc_w_h\n",
    "    boty = y + h + inc_h_h\n",
    "    \n",
    "    # Make sure values stay within range\n",
    "    if topx < 0:\n",
    "        topx = 0\n",
    "    if topy < 0:\n",
    "        topy = 0\n",
    "    if botx > cv_img.shape[1]:\n",
    "        botx = cv_img.shape[1]\n",
    "    if boty > cv_img.shape[0]:\n",
    "        boty = cv_img.shape[0]\n",
    "    \n",
    "    pt1 = (topx, topy)\n",
    "    pt2 = (botx, boty)\n",
    "\n",
    "    if debug:\n",
    "        cv2.rectangle(viz_img, pt1, pt2, (255, 0, 0), 3)    \n",
    "\n",
    "        cv2.drawContours(viz_img, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "        plt.imshow(filter1)\n",
    "        plt.title(f\"Cropped to `DIST_THRESHOLD={DIST_THRESHOLD}` pixels\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(filter1_8u, cmap='gray')\n",
    "        plt.title(f\"Unsigned 8 image\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(viz_img, cmap='gray')\n",
    "        plt.title(f\"Viz image\")\n",
    "        plt.show()\n",
    "\n",
    "    return cv_img, (filter1, pt1, pt2)\n",
    "    \n",
    "def findBestFrameROI(depth_imgs, search_max=30):\n",
    "    _index = -1\n",
    "    if len(depth_imgs) < search_max:\n",
    "        search_n = len(depth_imgs) - 1\n",
    "    else:\n",
    "        search_n = search_max\n",
    "        \n",
    "    for _ in range(search_n):\n",
    "        depth_img, ret = computeObjectBoundingBox(depth_imgs[_index], debug=False)\n",
    "        \n",
    "        #plt.imshow(depth_img)\n",
    "        #plt.show()\n",
    "        \n",
    "        if ret != None:\n",
    "            filter1, (topx, topy), (botx, boty) = ret\n",
    "\n",
    "            roi = filter1[topy:boty, topx:botx]\n",
    "            \n",
    "            return roi\n",
    "\n",
    "            #plt.imshow(filter1)\n",
    "            #plt.show()\n",
    "\n",
    "            #plt.imshow(roi)\n",
    "            #plt.show()\n",
    "            #print('#'*80)\n",
    "\n",
    "        _index += -1\n",
    "        \n",
    "    #print(\"No roi for this sample.\")\n",
    "    return None\n",
    "                     \n",
    "# The capacity of the containers in mL\n",
    "ANNOTATION = {1: 520.0, # Red cup\n",
    "              2: 185.0, # Small white cup\n",
    "              3: 202.0, # Small transparent cup\n",
    "              4: 296.0, # Green glass\n",
    "              5: 363.0, # Wine glass\n",
    "              6: 128.0, # Champagne flute\n",
    "              7: 3209.397, # Cereal box\n",
    "              8: 1239.84, # Biscuits box\n",
    "              9: 471.6} # Tea box\n",
    "\n",
    "ANNOTATION_MEAN = 734.981888888889\n",
    "ANNOTATION_STD = 929.7103190574344\n",
    "\n",
    "IMG_WIDTH = 1280\n",
    "IMG_HEIGHT = 720\n",
    "MAX_VAL = 700 # Anything larger than this distance in the images, is discarded\n",
    "\n",
    "total = 0\n",
    "failed = 0\n",
    "failed_samples = []\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for obj_id in range(1, 10):\n",
    "    print(f\"Extracting data from object id: `{obj_id}`\")\n",
    "    for sit in s_dict.keys():\n",
    "        for fi in fi_dict.keys():\n",
    "            for fu in fu_dict.keys():\n",
    "                for b in b_dict.keys():\n",
    "                    for l in l_dict.keys():\n",
    "                        try:\n",
    "                            sample = retrieve_data(obj_id, s=sit, fi=fi, fu=fu, b=b, l=l)\n",
    "                        except Exception as e:\n",
    "                            #print(f\"Failed...: {(obj_id, sit, fi, fu, b, l)}\")\n",
    "                            pass\n",
    "                        if sample != -1:\n",
    "                            ret = findBestFrameROI(sample['depth'][2])\n",
    "                            total += 1\n",
    "                            if ret is None:\n",
    "                                print(f\"Failed for {sample['depth'][2][-1]}\")\n",
    "                                failed += 1\n",
    "                                failed_samples.append((sit, fi, fu, b, l))\n",
    "                            else:\n",
    "                                # Add sample to dataset\n",
    "                                roi_h, roi_w = ret.shape\n",
    "                                \n",
    "                                # Normalize ROI by max val\n",
    "                                data.append((np.divide(ret, MAX_VAL), roi_h/IMG_HEIGHT, roi_w/IMG_WIDTH))\n",
    "\n",
    "                                # Get label data\n",
    "                                labels.append(ANNOTATION[obj_id] / 4000)\n",
    "\n",
    "print(len(data), len(labels))\n",
    "print(f\"Out of {total} samples, {failed} failed to find object ROI\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum_val = 0.0\n",
    "# min_val = 10000000000\n",
    "# for i in images:\n",
    "#     _max = i.max()\n",
    "#     _min = i.min()\n",
    "#     if _max > maximum_val:\n",
    "#         maximum_val = _max\n",
    "#     if _min < min_val:\n",
    "#         min_val = _min\n",
    "        \n",
    "# print(maximum_val, min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032      0.04625    0.0505     0.074      0.09075    0.1179\n",
      " 0.13       0.30996    0.80234925] [84 84 83 84 82 58 84 57 56]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032      0.04625    0.0505     0.074      0.09075    0.1179\n",
      " 0.13       0.30996    0.80234925] [73 75 70 66 69 50 73 48 47]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032      0.04625    0.0505     0.074      0.09075    0.1179\n",
      " 0.13       0.30996    0.80234925] [11  9 13 18 13  8 11  9  9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "import torch\n",
    "\n",
    "NET_SIZE = (112, 112)\n",
    "\n",
    "# Analyze class distribution and \n",
    "_classes, counts = np.unique(labels, return_counts=True)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Whole dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                   test_size=0.15)\n",
    "\n",
    "_classes, counts = np.unique(y_train, return_counts=True)\n",
    "n_train_samples = len(y_train)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Train dataset\")\n",
    "plt.show()\n",
    "\n",
    "_classes, counts = np.unique(y_test, return_counts=True)\n",
    "print(_classes, counts)\n",
    "plt.bar(_classes, counts)\n",
    "plt.title(\"Test dataset\")\n",
    "plt.show()\n",
    "\n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.images = []\n",
    "        self.rois_size = []\n",
    "        \n",
    "        for _x in x:\n",
    "            self.images.append(cv2.resize(_x[0], (112, 112)))\n",
    "            self.rois_size.append((_x[1], _x[2]))\n",
    "        \n",
    "        self.y_volume = y\n",
    "        \n",
    "        assert len(self.images) == len(self.rois_size)\n",
    "        assert len(self.images) == len(self.y_volume)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.Tensor(self.images[idx]).unsqueeze(0), torch.Tensor(self.rois_size[idx])), torch.Tensor([self.y_volume[idx]])\n",
    "    \n",
    "train_dataset = VolumeDataset(X_train, y_train)\n",
    "test_dataset = VolumeDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,\n",
    "                          num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "#for (img, roi_info), volume in test_loader:\n",
    "#    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[1, 15]: MSE Loss: 0.2461\n",
      "[1, 30]: MSE Loss: 0.1579\n",
      "[1, 45]: MSE Loss: 0.1360\n",
      "[1, 60]: MSE Loss: 0.1265\n",
      "Epoch 2\n",
      "[2, 15]: MSE Loss: 0.0988\n",
      "[2, 30]: MSE Loss: 0.0797\n",
      "[2, 45]: MSE Loss: 0.0746\n",
      "[2, 60]: MSE Loss: 0.0777\n",
      "Epoch 3\n",
      "[3, 15]: MSE Loss: 0.0657\n",
      "[3, 30]: MSE Loss: 0.0579\n",
      "[3, 45]: MSE Loss: 0.0508\n",
      "[3, 60]: MSE Loss: 0.0619\n",
      "Epoch 4\n",
      "[4, 15]: MSE Loss: 0.0386\n",
      "[4, 30]: MSE Loss: 0.0397\n",
      "[4, 45]: MSE Loss: 0.0426\n",
      "[4, 60]: MSE Loss: 0.0361\n",
      "Epoch 5\n",
      "[5, 15]: MSE Loss: 0.0352\n",
      "[5, 30]: MSE Loss: 0.0285\n",
      "[5, 45]: MSE Loss: 0.0248\n",
      "[5, 60]: MSE Loss: 0.0279\n",
      "Epoch 6\n",
      "[6, 15]: MSE Loss: 0.0256\n",
      "[6, 30]: MSE Loss: 0.0176\n",
      "[6, 45]: MSE Loss: 0.0159\n",
      "[6, 60]: MSE Loss: 0.0207\n",
      "Epoch 7\n",
      "[7, 15]: MSE Loss: 0.0167\n",
      "[7, 30]: MSE Loss: 0.0150\n",
      "[7, 45]: MSE Loss: 0.0143\n",
      "[7, 60]: MSE Loss: 0.0182\n",
      "Epoch 8\n",
      "[8, 15]: MSE Loss: 0.0185\n",
      "[8, 30]: MSE Loss: 0.0127\n",
      "[8, 45]: MSE Loss: 0.0196\n",
      "[8, 60]: MSE Loss: 0.0115\n",
      "Epoch 9\n",
      "[9, 15]: MSE Loss: 0.0207\n",
      "[9, 30]: MSE Loss: 0.0240\n",
      "[9, 45]: MSE Loss: 0.0245\n",
      "[9, 60]: MSE Loss: 0.0428\n",
      "Epoch 10\n",
      "[10, 15]: MSE Loss: 0.0388\n",
      "[10, 30]: MSE Loss: 0.0254\n",
      "[10, 45]: MSE Loss: 0.0239\n",
      "[10, 60]: MSE Loss: 0.0294\n",
      "Test Loss: 0.05751\n",
      "########################################\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntnuerc/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 15]: MSE Loss: 0.0152\n",
      "[11, 30]: MSE Loss: 0.0157\n",
      "[11, 45]: MSE Loss: 0.0210\n",
      "[11, 60]: MSE Loss: 0.0152\n",
      "Epoch 12\n",
      "[12, 15]: MSE Loss: 0.0193\n",
      "[12, 30]: MSE Loss: 0.0134\n",
      "[12, 45]: MSE Loss: 0.0139\n",
      "[12, 60]: MSE Loss: 0.0103\n",
      "Epoch 13\n",
      "[13, 15]: MSE Loss: 0.0112\n",
      "[13, 30]: MSE Loss: 0.0209\n",
      "[13, 45]: MSE Loss: 0.0156\n",
      "[13, 60]: MSE Loss: 0.0139\n",
      "Epoch 14\n",
      "[14, 15]: MSE Loss: 0.0124\n",
      "[14, 30]: MSE Loss: 0.0257\n",
      "[14, 45]: MSE Loss: 0.0230\n",
      "[14, 60]: MSE Loss: 0.0206\n",
      "Epoch 15\n",
      "[15, 15]: MSE Loss: 0.0120\n",
      "[15, 30]: MSE Loss: 0.0118\n",
      "[15, 45]: MSE Loss: 0.0162\n",
      "[15, 60]: MSE Loss: 0.0129\n",
      "Epoch 16\n",
      "[16, 15]: MSE Loss: 0.0126\n",
      "[16, 30]: MSE Loss: 0.0118\n",
      "[16, 45]: MSE Loss: 0.0112\n",
      "[16, 60]: MSE Loss: 0.0087\n",
      "Epoch 17\n",
      "[17, 15]: MSE Loss: 0.0086\n",
      "[17, 30]: MSE Loss: 0.0086\n",
      "[17, 45]: MSE Loss: 0.0100\n",
      "[17, 60]: MSE Loss: 0.0133\n",
      "Epoch 18\n",
      "[18, 15]: MSE Loss: 0.0084\n",
      "[18, 30]: MSE Loss: 0.0072\n",
      "[18, 45]: MSE Loss: 0.0076\n",
      "[18, 60]: MSE Loss: 0.0082\n",
      "Epoch 19\n",
      "[19, 15]: MSE Loss: 0.0077\n",
      "[19, 30]: MSE Loss: 0.0080\n",
      "[19, 45]: MSE Loss: 0.0064\n",
      "[19, 60]: MSE Loss: 0.0067\n",
      "Epoch 20\n",
      "[20, 15]: MSE Loss: 0.0081\n",
      "[20, 30]: MSE Loss: 0.0079\n",
      "[20, 45]: MSE Loss: 0.0074\n",
      "[20, 60]: MSE Loss: 0.0062\n",
      "Test Loss: 0.06350\n",
      "########################################\n",
      "Epoch 21\n",
      "[21, 15]: MSE Loss: 0.0091\n",
      "[21, 30]: MSE Loss: 0.0081\n",
      "[21, 45]: MSE Loss: 0.0070\n",
      "[21, 60]: MSE Loss: 0.0096\n",
      "Epoch 22\n",
      "[22, 15]: MSE Loss: 0.0081\n",
      "[22, 30]: MSE Loss: 0.0077\n",
      "[22, 45]: MSE Loss: 0.0103\n",
      "[22, 60]: MSE Loss: 0.0120\n",
      "Epoch 23\n",
      "[23, 15]: MSE Loss: 0.0081\n",
      "[23, 30]: MSE Loss: 0.0097\n",
      "[23, 45]: MSE Loss: 0.0066\n",
      "[23, 60]: MSE Loss: 0.0083\n",
      "Epoch 24\n",
      "[24, 15]: MSE Loss: 0.0167\n",
      "[24, 30]: MSE Loss: 0.0078\n",
      "[24, 45]: MSE Loss: 0.0058\n",
      "[24, 60]: MSE Loss: 0.0075\n",
      "Epoch 25\n",
      "[25, 15]: MSE Loss: 0.0089\n",
      "[25, 30]: MSE Loss: 0.0133\n",
      "[25, 45]: MSE Loss: 0.0133\n",
      "[25, 60]: MSE Loss: 0.0080\n",
      "Epoch 26\n",
      "[26, 15]: MSE Loss: 0.0074\n",
      "[26, 30]: MSE Loss: 0.0088\n",
      "[26, 45]: MSE Loss: 0.0069\n",
      "[26, 60]: MSE Loss: 0.0048\n",
      "Epoch 27\n",
      "[27, 15]: MSE Loss: 0.0096\n",
      "[27, 30]: MSE Loss: 0.0083\n",
      "[27, 45]: MSE Loss: 0.0061\n",
      "[27, 60]: MSE Loss: 0.0043\n",
      "Epoch 28\n",
      "[28, 15]: MSE Loss: 0.0092\n",
      "[28, 30]: MSE Loss: 0.0081\n",
      "[28, 45]: MSE Loss: 0.0048\n",
      "[28, 60]: MSE Loss: 0.0083\n",
      "Epoch 29\n",
      "[29, 15]: MSE Loss: 0.0050\n",
      "[29, 30]: MSE Loss: 0.0083\n",
      "[29, 45]: MSE Loss: 0.0049\n",
      "[29, 60]: MSE Loss: 0.0049\n",
      "Epoch 30\n",
      "[30, 15]: MSE Loss: 0.0068\n",
      "[30, 30]: MSE Loss: 0.0095\n",
      "[30, 45]: MSE Loss: 0.0059\n",
      "[30, 60]: MSE Loss: 0.0073\n",
      "Test Loss: 0.05997\n",
      "########################################\n",
      "Epoch 31\n",
      "[31, 15]: MSE Loss: 0.0059\n",
      "[31, 30]: MSE Loss: 0.0061\n",
      "[31, 45]: MSE Loss: 0.0091\n",
      "[31, 60]: MSE Loss: 0.0059\n",
      "Epoch 32\n",
      "[32, 15]: MSE Loss: 0.0098\n",
      "[32, 30]: MSE Loss: 0.0033\n",
      "[32, 45]: MSE Loss: 0.0070\n",
      "[32, 60]: MSE Loss: 0.0045\n",
      "Epoch 33\n",
      "[33, 15]: MSE Loss: 0.0091\n",
      "[33, 30]: MSE Loss: 0.0053\n",
      "[33, 45]: MSE Loss: 0.0149\n",
      "[33, 60]: MSE Loss: 0.0144\n",
      "Epoch 34\n",
      "[34, 15]: MSE Loss: 0.0192\n",
      "[34, 30]: MSE Loss: 0.0178\n",
      "[34, 45]: MSE Loss: 0.0102\n",
      "[34, 60]: MSE Loss: 0.0083\n",
      "Epoch 35\n",
      "[35, 15]: MSE Loss: 0.0089\n",
      "[35, 30]: MSE Loss: 0.0101\n",
      "[35, 45]: MSE Loss: 0.0087\n",
      "[35, 60]: MSE Loss: 0.0087\n",
      "Epoch 36\n",
      "[36, 15]: MSE Loss: 0.0112\n",
      "[36, 30]: MSE Loss: 0.0091\n",
      "[36, 45]: MSE Loss: 0.0079\n",
      "[36, 60]: MSE Loss: 0.0051\n",
      "Epoch 37\n",
      "[37, 15]: MSE Loss: 0.0101\n",
      "[37, 30]: MSE Loss: 0.0055\n",
      "[37, 45]: MSE Loss: 0.0068\n",
      "[37, 60]: MSE Loss: 0.0060\n",
      "Epoch 38\n",
      "[38, 15]: MSE Loss: 0.0074\n",
      "[38, 30]: MSE Loss: 0.0079\n",
      "[38, 45]: MSE Loss: 0.0048\n",
      "[38, 60]: MSE Loss: 0.0068\n",
      "Epoch 39\n",
      "[39, 15]: MSE Loss: 0.0065\n",
      "[39, 30]: MSE Loss: 0.0055\n",
      "[39, 45]: MSE Loss: 0.0037\n",
      "[39, 60]: MSE Loss: 0.0077\n",
      "Epoch 40\n",
      "[40, 15]: MSE Loss: 0.0067\n",
      "[40, 30]: MSE Loss: 0.0047\n",
      "[40, 45]: MSE Loss: 0.0148\n",
      "[40, 60]: MSE Loss: 0.0157\n",
      "Test Loss: 0.06526\n",
      "########################################\n",
      "Epoch 41\n",
      "[41, 15]: MSE Loss: 0.0182\n",
      "[41, 30]: MSE Loss: 0.0117\n",
      "[41, 45]: MSE Loss: 0.0127\n",
      "[41, 60]: MSE Loss: 0.0170\n",
      "Epoch 42\n",
      "[42, 15]: MSE Loss: 0.0264\n",
      "[42, 30]: MSE Loss: 0.0299\n",
      "[42, 45]: MSE Loss: 0.0279\n",
      "[42, 60]: MSE Loss: 0.0139\n",
      "Epoch 43\n",
      "[43, 15]: MSE Loss: 0.0200\n",
      "[43, 30]: MSE Loss: 0.0120\n",
      "[43, 45]: MSE Loss: 0.0089\n",
      "[43, 60]: MSE Loss: 0.0115\n",
      "Epoch 44\n",
      "[44, 15]: MSE Loss: 0.0086\n",
      "[44, 30]: MSE Loss: 0.0099\n",
      "[44, 45]: MSE Loss: 0.0052\n",
      "[44, 60]: MSE Loss: 0.0066\n",
      "Epoch 45\n",
      "[45, 15]: MSE Loss: 0.0063\n",
      "[45, 30]: MSE Loss: 0.0046\n",
      "[45, 45]: MSE Loss: 0.0114\n",
      "[45, 60]: MSE Loss: 0.0087\n",
      "Epoch 46\n",
      "[46, 15]: MSE Loss: 0.0058\n",
      "[46, 30]: MSE Loss: 0.0057\n",
      "[46, 45]: MSE Loss: 0.0053\n",
      "[46, 60]: MSE Loss: 0.0067\n",
      "Epoch 47\n",
      "[47, 15]: MSE Loss: 0.0061\n",
      "[47, 30]: MSE Loss: 0.0106\n",
      "[47, 45]: MSE Loss: 0.0066\n",
      "[47, 60]: MSE Loss: 0.0084\n",
      "Epoch 48\n",
      "[48, 15]: MSE Loss: 0.0051\n",
      "[48, 30]: MSE Loss: 0.0052\n",
      "[48, 45]: MSE Loss: 0.0041\n",
      "[48, 60]: MSE Loss: 0.0081\n",
      "Epoch 49\n",
      "[49, 15]: MSE Loss: 0.0063\n",
      "[49, 30]: MSE Loss: 0.0047\n",
      "[49, 45]: MSE Loss: 0.0072\n",
      "[49, 60]: MSE Loss: 0.0047\n",
      "Epoch 50\n",
      "[50, 15]: MSE Loss: 0.0057\n",
      "[50, 30]: MSE Loss: 0.0111\n",
      "[50, 45]: MSE Loss: 0.0288\n",
      "[50, 60]: MSE Loss: 0.0088\n",
      "Test Loss: 0.07181\n",
      "########################################\n",
      "Epoch 51\n",
      "[51, 15]: MSE Loss: 0.0080\n",
      "[51, 30]: MSE Loss: 0.0039\n",
      "[51, 45]: MSE Loss: 0.0074\n",
      "[51, 60]: MSE Loss: 0.0061\n",
      "Epoch 52\n",
      "[52, 15]: MSE Loss: 0.0063\n",
      "[52, 30]: MSE Loss: 0.0072\n",
      "[52, 45]: MSE Loss: 0.0055\n",
      "[52, 60]: MSE Loss: 0.0057\n",
      "Epoch 53\n",
      "[53, 15]: MSE Loss: 0.0055\n",
      "[53, 30]: MSE Loss: 0.0054\n",
      "[53, 45]: MSE Loss: 0.0065\n",
      "[53, 60]: MSE Loss: 0.0038\n",
      "Epoch 54\n",
      "[54, 15]: MSE Loss: 0.0046\n",
      "[54, 30]: MSE Loss: 0.0105\n",
      "[54, 45]: MSE Loss: 0.0058\n",
      "[54, 60]: MSE Loss: 0.0049\n",
      "Epoch 55\n",
      "[55, 15]: MSE Loss: 0.0054\n",
      "[55, 30]: MSE Loss: 0.0060\n",
      "[55, 45]: MSE Loss: 0.0069\n",
      "[55, 60]: MSE Loss: 0.0027\n",
      "Epoch 56\n",
      "[56, 15]: MSE Loss: 0.0075\n",
      "[56, 30]: MSE Loss: 0.0043\n",
      "[56, 45]: MSE Loss: 0.0038\n",
      "[56, 60]: MSE Loss: 0.0079\n",
      "Epoch 57\n",
      "[57, 15]: MSE Loss: 0.0091\n",
      "[57, 30]: MSE Loss: 0.0047\n",
      "[57, 45]: MSE Loss: 0.0055\n",
      "[57, 60]: MSE Loss: 0.0037\n",
      "Epoch 58\n",
      "[58, 15]: MSE Loss: 0.0031\n",
      "[58, 30]: MSE Loss: 0.0065\n",
      "[58, 45]: MSE Loss: 0.0046\n",
      "[58, 60]: MSE Loss: 0.0034\n",
      "Epoch 59\n",
      "[59, 15]: MSE Loss: 0.0041\n",
      "[59, 30]: MSE Loss: 0.0038\n",
      "[59, 45]: MSE Loss: 0.0038\n",
      "[59, 60]: MSE Loss: 0.0040\n",
      "Epoch 60\n",
      "[60, 15]: MSE Loss: 0.0026\n",
      "[60, 30]: MSE Loss: 0.0027\n",
      "[60, 45]: MSE Loss: 0.0047\n",
      "[60, 60]: MSE Loss: 0.0038\n",
      "Test Loss: 0.05147\n",
      "########################################\n",
      "Epoch 61\n",
      "[61, 15]: MSE Loss: 0.0041\n",
      "[61, 30]: MSE Loss: 0.0033\n",
      "[61, 45]: MSE Loss: 0.0034\n",
      "[61, 60]: MSE Loss: 0.0078\n",
      "Epoch 62\n",
      "[62, 15]: MSE Loss: 0.0073\n",
      "[62, 30]: MSE Loss: 0.0053\n",
      "[62, 45]: MSE Loss: 0.0028\n",
      "[62, 60]: MSE Loss: 0.0041\n",
      "Epoch 63\n",
      "[63, 15]: MSE Loss: 0.0018\n",
      "[63, 30]: MSE Loss: 0.0047\n",
      "[63, 45]: MSE Loss: 0.0068\n",
      "[63, 60]: MSE Loss: 0.0054\n",
      "Epoch 64\n",
      "[64, 15]: MSE Loss: 0.0161\n",
      "[64, 30]: MSE Loss: 0.0050\n",
      "[64, 45]: MSE Loss: 0.0057\n",
      "[64, 60]: MSE Loss: 0.0043\n",
      "Epoch 65\n",
      "[65, 15]: MSE Loss: 0.0054\n",
      "[65, 30]: MSE Loss: 0.0055\n",
      "[65, 45]: MSE Loss: 0.0046\n",
      "[65, 60]: MSE Loss: 0.0042\n",
      "Epoch 66\n",
      "[66, 15]: MSE Loss: 0.0071\n",
      "[66, 30]: MSE Loss: 0.0038\n",
      "[66, 45]: MSE Loss: 0.0034\n",
      "[66, 60]: MSE Loss: 0.0049\n",
      "Epoch 67\n",
      "[67, 15]: MSE Loss: 0.0060\n",
      "[67, 30]: MSE Loss: 0.0065\n",
      "[67, 45]: MSE Loss: 0.0058\n",
      "[67, 60]: MSE Loss: 0.0039\n",
      "Epoch 68\n",
      "[68, 15]: MSE Loss: 0.0035\n",
      "[68, 30]: MSE Loss: 0.0074\n",
      "[68, 45]: MSE Loss: 0.0065\n",
      "[68, 60]: MSE Loss: 0.0049\n",
      "Epoch 69\n",
      "[69, 15]: MSE Loss: 0.0056\n",
      "[69, 30]: MSE Loss: 0.0039\n",
      "[69, 45]: MSE Loss: 0.0046\n",
      "[69, 60]: MSE Loss: 0.0029\n",
      "Epoch 70\n",
      "[70, 15]: MSE Loss: 0.0039\n",
      "[70, 30]: MSE Loss: 0.0044\n",
      "[70, 45]: MSE Loss: 0.0042\n",
      "[70, 60]: MSE Loss: 0.0052\n",
      "Test Loss: 0.05282\n",
      "########################################\n",
      "Epoch 71\n",
      "[71, 15]: MSE Loss: 0.0074\n",
      "[71, 30]: MSE Loss: 0.0034\n",
      "[71, 45]: MSE Loss: 0.0045\n",
      "[71, 60]: MSE Loss: 0.0047\n",
      "Epoch 72\n",
      "[72, 15]: MSE Loss: 0.0086\n",
      "[72, 30]: MSE Loss: 0.0038\n",
      "[72, 45]: MSE Loss: 0.0031\n",
      "[72, 60]: MSE Loss: 0.0032\n",
      "Epoch 73\n",
      "[73, 15]: MSE Loss: 0.0031\n",
      "[73, 30]: MSE Loss: 0.0060\n",
      "[73, 45]: MSE Loss: 0.0049\n",
      "[73, 60]: MSE Loss: 0.0021\n",
      "Epoch 74\n",
      "[74, 15]: MSE Loss: 0.0025\n",
      "[74, 30]: MSE Loss: 0.0026\n",
      "[74, 45]: MSE Loss: 0.0042\n",
      "[74, 60]: MSE Loss: 0.0041\n",
      "Epoch 75\n",
      "[75, 15]: MSE Loss: 0.0029\n",
      "[75, 30]: MSE Loss: 0.0041\n",
      "[75, 45]: MSE Loss: 0.0045\n",
      "[75, 60]: MSE Loss: 0.0043\n",
      "Epoch 76\n",
      "[76, 15]: MSE Loss: 0.0038\n",
      "[76, 30]: MSE Loss: 0.0029\n",
      "[76, 45]: MSE Loss: 0.0036\n",
      "[76, 60]: MSE Loss: 0.0074\n",
      "Epoch 77\n",
      "[77, 15]: MSE Loss: 0.0037\n",
      "[77, 30]: MSE Loss: 0.0039\n",
      "[77, 45]: MSE Loss: 0.0033\n",
      "[77, 60]: MSE Loss: 0.0028\n",
      "Epoch 78\n",
      "[78, 15]: MSE Loss: 0.0036\n",
      "[78, 30]: MSE Loss: 0.0051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 45]: MSE Loss: 0.0026\n",
      "[78, 60]: MSE Loss: 0.0042\n",
      "Epoch 79\n",
      "[79, 15]: MSE Loss: 0.0081\n",
      "[79, 30]: MSE Loss: 0.0024\n",
      "[79, 45]: MSE Loss: 0.0032\n",
      "[79, 60]: MSE Loss: 0.0064\n",
      "Epoch 80\n",
      "[80, 15]: MSE Loss: 0.0101\n",
      "[80, 30]: MSE Loss: 0.0062\n",
      "[80, 45]: MSE Loss: 0.0036\n",
      "[80, 60]: MSE Loss: 0.0065\n",
      "Test Loss: 0.05726\n",
      "########################################\n",
      "Epoch 81\n",
      "[81, 15]: MSE Loss: 0.0071\n",
      "[81, 30]: MSE Loss: 0.0050\n",
      "[81, 45]: MSE Loss: 0.0046\n",
      "[81, 60]: MSE Loss: 0.0048\n",
      "Epoch 82\n",
      "[82, 15]: MSE Loss: 0.0083\n",
      "[82, 30]: MSE Loss: 0.0039\n",
      "[82, 45]: MSE Loss: 0.0032\n",
      "[82, 60]: MSE Loss: 0.0078\n",
      "Epoch 83\n",
      "[83, 15]: MSE Loss: 0.0034\n",
      "[83, 30]: MSE Loss: 0.0087\n",
      "[83, 45]: MSE Loss: 0.0033\n",
      "[83, 60]: MSE Loss: 0.0057\n",
      "Epoch 84\n",
      "[84, 15]: MSE Loss: 0.0052\n",
      "[84, 30]: MSE Loss: 0.0052\n",
      "[84, 45]: MSE Loss: 0.0068\n",
      "[84, 60]: MSE Loss: 0.0052\n",
      "Epoch 85\n",
      "[85, 15]: MSE Loss: 0.0050\n",
      "[85, 30]: MSE Loss: 0.0052\n",
      "[85, 45]: MSE Loss: 0.0043\n",
      "[85, 60]: MSE Loss: 0.0045\n",
      "Epoch 86\n",
      "[86, 15]: MSE Loss: 0.0047\n",
      "[86, 30]: MSE Loss: 0.0042\n",
      "[86, 45]: MSE Loss: 0.0047\n",
      "[86, 60]: MSE Loss: 0.0056\n",
      "Epoch 87\n",
      "[87, 15]: MSE Loss: 0.0031\n",
      "[87, 30]: MSE Loss: 0.0026\n",
      "[87, 45]: MSE Loss: 0.0046\n",
      "[87, 60]: MSE Loss: 0.0054\n",
      "Epoch 88\n",
      "[88, 15]: MSE Loss: 0.0025\n",
      "[88, 30]: MSE Loss: 0.0094\n",
      "[88, 45]: MSE Loss: 0.0034\n",
      "[88, 60]: MSE Loss: 0.0035\n",
      "Epoch 89\n",
      "[89, 15]: MSE Loss: 0.0025\n",
      "[89, 30]: MSE Loss: 0.0029\n",
      "[89, 45]: MSE Loss: 0.0033\n",
      "[89, 60]: MSE Loss: 0.0037\n",
      "Epoch 90\n",
      "[90, 15]: MSE Loss: 0.0017\n",
      "[90, 30]: MSE Loss: 0.0039\n",
      "[90, 45]: MSE Loss: 0.0030\n",
      "[90, 60]: MSE Loss: 0.0021\n",
      "Test Loss: 0.05118\n",
      "########################################\n",
      "Epoch 91\n",
      "[91, 15]: MSE Loss: 0.0020\n",
      "[91, 30]: MSE Loss: 0.0037\n",
      "[91, 45]: MSE Loss: 0.0031\n",
      "[91, 60]: MSE Loss: 0.0039\n",
      "Epoch 92\n",
      "[92, 15]: MSE Loss: 0.0028\n",
      "[92, 30]: MSE Loss: 0.0044\n",
      "[92, 45]: MSE Loss: 0.0028\n",
      "[92, 60]: MSE Loss: 0.0028\n",
      "Epoch 93\n",
      "[93, 15]: MSE Loss: 0.0031\n",
      "[93, 30]: MSE Loss: 0.0035\n",
      "[93, 45]: MSE Loss: 0.0044\n",
      "[93, 60]: MSE Loss: 0.0025\n",
      "Epoch 94\n",
      "[94, 15]: MSE Loss: 0.0051\n",
      "[94, 30]: MSE Loss: 0.0029\n",
      "[94, 45]: MSE Loss: 0.0028\n",
      "[94, 60]: MSE Loss: 0.0044\n",
      "Epoch 95\n",
      "[95, 15]: MSE Loss: 0.0083\n",
      "[95, 30]: MSE Loss: 0.0128\n",
      "[95, 45]: MSE Loss: 0.0078\n",
      "[95, 60]: MSE Loss: 0.0112\n",
      "Epoch 96\n",
      "[96, 15]: MSE Loss: 0.0070\n",
      "[96, 30]: MSE Loss: 0.0105\n",
      "[96, 45]: MSE Loss: 0.0082\n",
      "[96, 60]: MSE Loss: 0.0052\n",
      "Epoch 97\n",
      "[97, 15]: MSE Loss: 0.0100\n",
      "[97, 30]: MSE Loss: 0.0048\n",
      "[97, 45]: MSE Loss: 0.0074\n",
      "[97, 60]: MSE Loss: 0.0051\n",
      "Epoch 98\n",
      "[98, 15]: MSE Loss: 0.0063\n",
      "[98, 30]: MSE Loss: 0.0104\n",
      "[98, 45]: MSE Loss: 0.0067\n",
      "[98, 60]: MSE Loss: 0.0049\n",
      "Epoch 99\n",
      "[99, 15]: MSE Loss: 0.0043\n",
      "[99, 30]: MSE Loss: 0.0057\n",
      "[99, 45]: MSE Loss: 0.0069\n",
      "[99, 60]: MSE Loss: 0.0047\n",
      "Epoch 100\n",
      "[100, 15]: MSE Loss: 0.0027\n",
      "[100, 30]: MSE Loss: 0.0074\n",
      "[100, 45]: MSE Loss: 0.0050\n",
      "[100, 60]: MSE Loss: 0.0042\n",
      "Test Loss: 0.04839\n",
      "########################################\n",
      "Epoch 101\n",
      "[101, 15]: MSE Loss: 0.0028\n",
      "[101, 30]: MSE Loss: 0.0040\n",
      "[101, 45]: MSE Loss: 0.0054\n",
      "[101, 60]: MSE Loss: 0.0041\n",
      "Epoch 102\n",
      "[102, 15]: MSE Loss: 0.0045\n",
      "[102, 30]: MSE Loss: 0.0040\n",
      "[102, 45]: MSE Loss: 0.0027\n",
      "[102, 60]: MSE Loss: 0.0032\n",
      "Epoch 103\n",
      "[103, 15]: MSE Loss: 0.0047\n",
      "[103, 30]: MSE Loss: 0.0047\n",
      "[103, 45]: MSE Loss: 0.0031\n",
      "[103, 60]: MSE Loss: 0.0023\n",
      "Epoch 104\n",
      "[104, 15]: MSE Loss: 0.0030\n",
      "[104, 30]: MSE Loss: 0.0047\n",
      "[104, 45]: MSE Loss: 0.0035\n",
      "[104, 60]: MSE Loss: 0.0059\n",
      "Epoch 105\n",
      "[105, 15]: MSE Loss: 0.0054\n",
      "[105, 30]: MSE Loss: 0.0078\n",
      "[105, 45]: MSE Loss: 0.0028\n",
      "[105, 60]: MSE Loss: 0.0060\n",
      "Epoch 106\n",
      "[106, 15]: MSE Loss: 0.0026\n",
      "[106, 30]: MSE Loss: 0.0044\n",
      "[106, 45]: MSE Loss: 0.0034\n",
      "[106, 60]: MSE Loss: 0.0038\n",
      "Epoch 107\n",
      "[107, 15]: MSE Loss: 0.0046\n",
      "[107, 30]: MSE Loss: 0.0020\n",
      "[107, 45]: MSE Loss: 0.0064\n",
      "[107, 60]: MSE Loss: 0.0056\n",
      "Epoch 108\n",
      "[108, 15]: MSE Loss: 0.0041\n",
      "[108, 30]: MSE Loss: 0.0087\n",
      "[108, 45]: MSE Loss: 0.0045\n",
      "[108, 60]: MSE Loss: 0.0057\n",
      "Epoch 109\n",
      "[109, 15]: MSE Loss: 0.0038\n",
      "[109, 30]: MSE Loss: 0.0097\n",
      "[109, 45]: MSE Loss: 0.0058\n",
      "[109, 60]: MSE Loss: 0.0034\n",
      "Epoch 110\n",
      "[110, 15]: MSE Loss: 0.0040\n",
      "[110, 30]: MSE Loss: 0.0041\n",
      "[110, 45]: MSE Loss: 0.0040\n",
      "[110, 60]: MSE Loss: 0.0050\n",
      "Test Loss: 0.06163\n",
      "########################################\n",
      "Epoch 111\n",
      "[111, 15]: MSE Loss: 0.0037\n",
      "[111, 30]: MSE Loss: 0.0033\n",
      "[111, 45]: MSE Loss: 0.0043\n",
      "[111, 60]: MSE Loss: 0.0050\n",
      "Epoch 112\n",
      "[112, 15]: MSE Loss: 0.0037\n",
      "[112, 30]: MSE Loss: 0.0036\n",
      "[112, 45]: MSE Loss: 0.0042\n",
      "[112, 60]: MSE Loss: 0.0038\n",
      "Epoch 113\n",
      "[113, 15]: MSE Loss: 0.0037\n",
      "[113, 30]: MSE Loss: 0.0030\n",
      "[113, 45]: MSE Loss: 0.0052\n",
      "[113, 60]: MSE Loss: 0.0028\n",
      "Epoch 114\n",
      "[114, 15]: MSE Loss: 0.0060\n",
      "[114, 30]: MSE Loss: 0.0037\n",
      "[114, 45]: MSE Loss: 0.0029\n",
      "[114, 60]: MSE Loss: 0.0018\n",
      "Epoch 115\n",
      "[115, 15]: MSE Loss: 0.0026\n",
      "[115, 30]: MSE Loss: 0.0025\n",
      "[115, 45]: MSE Loss: 0.0037\n",
      "[115, 60]: MSE Loss: 0.0055\n",
      "Epoch 116\n",
      "[116, 15]: MSE Loss: 0.0018\n",
      "[116, 30]: MSE Loss: 0.0032\n",
      "[116, 45]: MSE Loss: 0.0034\n",
      "[116, 60]: MSE Loss: 0.0018\n",
      "Epoch 117\n",
      "[117, 15]: MSE Loss: 0.0033\n",
      "[117, 30]: MSE Loss: 0.0026\n",
      "[117, 45]: MSE Loss: 0.0053\n",
      "[117, 60]: MSE Loss: 0.0028\n",
      "Epoch 118\n",
      "[118, 15]: MSE Loss: 0.0040\n",
      "[118, 30]: MSE Loss: 0.0027\n",
      "[118, 45]: MSE Loss: 0.0085\n",
      "[118, 60]: MSE Loss: 0.0056\n",
      "Epoch 119\n",
      "[119, 15]: MSE Loss: 0.0023\n",
      "[119, 30]: MSE Loss: 0.0029\n",
      "[119, 45]: MSE Loss: 0.0045\n",
      "[119, 60]: MSE Loss: 0.0058\n",
      "Epoch 120\n",
      "[120, 15]: MSE Loss: 0.0028\n",
      "[120, 30]: MSE Loss: 0.0043\n",
      "[120, 45]: MSE Loss: 0.0034\n",
      "[120, 60]: MSE Loss: 0.0030\n",
      "Test Loss: 0.09691\n",
      "########################################\n",
      "Epoch 121\n",
      "[121, 15]: MSE Loss: 0.0033\n",
      "[121, 30]: MSE Loss: 0.0036\n",
      "[121, 45]: MSE Loss: 0.0069\n",
      "[121, 60]: MSE Loss: 0.0027\n",
      "Epoch 122\n",
      "[122, 15]: MSE Loss: 0.0024\n",
      "[122, 30]: MSE Loss: 0.0041\n",
      "[122, 45]: MSE Loss: 0.0047\n",
      "[122, 60]: MSE Loss: 0.0081\n",
      "Epoch 123\n",
      "[123, 15]: MSE Loss: 0.0020\n",
      "[123, 30]: MSE Loss: 0.0042\n",
      "[123, 45]: MSE Loss: 0.0038\n",
      "[123, 60]: MSE Loss: 0.0048\n",
      "Epoch 124\n",
      "[124, 15]: MSE Loss: 0.0035\n",
      "[124, 30]: MSE Loss: 0.0058\n",
      "[124, 45]: MSE Loss: 0.0045\n",
      "[124, 60]: MSE Loss: 0.0040\n",
      "Epoch 125\n",
      "[125, 15]: MSE Loss: 0.0051\n",
      "[125, 30]: MSE Loss: 0.0036\n",
      "[125, 45]: MSE Loss: 0.0033\n",
      "[125, 60]: MSE Loss: 0.0021\n",
      "Epoch 126\n",
      "[126, 15]: MSE Loss: 0.0056\n",
      "[126, 30]: MSE Loss: 0.0030\n",
      "[126, 45]: MSE Loss: 0.0085\n",
      "[126, 60]: MSE Loss: 0.0060\n",
      "Epoch 127\n",
      "[127, 15]: MSE Loss: 0.0075\n",
      "[127, 30]: MSE Loss: 0.0052\n",
      "[127, 45]: MSE Loss: 0.0024\n",
      "[127, 60]: MSE Loss: 0.0034\n",
      "Epoch 128\n",
      "[128, 15]: MSE Loss: 0.0020\n",
      "[128, 30]: MSE Loss: 0.0107\n",
      "[128, 45]: MSE Loss: 0.0031\n",
      "[128, 60]: MSE Loss: 0.0042\n",
      "Epoch 129\n",
      "[129, 15]: MSE Loss: 0.0036\n",
      "[129, 30]: MSE Loss: 0.0047\n",
      "[129, 45]: MSE Loss: 0.0044\n",
      "[129, 60]: MSE Loss: 0.0042\n",
      "Epoch 130\n",
      "[130, 15]: MSE Loss: 0.0043\n",
      "[130, 30]: MSE Loss: 0.0035\n",
      "[130, 45]: MSE Loss: 0.0020\n",
      "[130, 60]: MSE Loss: 0.0036\n",
      "Test Loss: 0.10460\n",
      "########################################\n",
      "Epoch 131\n",
      "[131, 15]: MSE Loss: 0.0043\n",
      "[131, 30]: MSE Loss: 0.0078\n",
      "[131, 45]: MSE Loss: 0.0030\n",
      "[131, 60]: MSE Loss: 0.0070\n",
      "Epoch 132\n",
      "[132, 15]: MSE Loss: 0.0054\n",
      "[132, 30]: MSE Loss: 0.0037\n",
      "[132, 45]: MSE Loss: 0.0040\n",
      "[132, 60]: MSE Loss: 0.0061\n",
      "Epoch 133\n",
      "[133, 15]: MSE Loss: 0.0087\n",
      "[133, 30]: MSE Loss: 0.0042\n",
      "[133, 45]: MSE Loss: 0.0026\n",
      "[133, 60]: MSE Loss: 0.0048\n",
      "Epoch 134\n",
      "[134, 15]: MSE Loss: 0.0019\n",
      "[134, 30]: MSE Loss: 0.0046\n",
      "[134, 45]: MSE Loss: 0.0044\n",
      "[134, 60]: MSE Loss: 0.0027\n",
      "Epoch 135\n",
      "[135, 15]: MSE Loss: 0.0027\n",
      "[135, 30]: MSE Loss: 0.0034\n",
      "[135, 45]: MSE Loss: 0.0028\n",
      "[135, 60]: MSE Loss: 0.0026\n",
      "Epoch 136\n",
      "[136, 15]: MSE Loss: 0.0035\n",
      "[136, 30]: MSE Loss: 0.0022\n",
      "[136, 45]: MSE Loss: 0.0027\n",
      "[136, 60]: MSE Loss: 0.0062\n",
      "Epoch 137\n",
      "[137, 15]: MSE Loss: 0.0073\n",
      "[137, 30]: MSE Loss: 0.0045\n",
      "[137, 45]: MSE Loss: 0.0049\n",
      "[137, 60]: MSE Loss: 0.0030\n",
      "Epoch 138\n",
      "[138, 15]: MSE Loss: 0.0029\n",
      "[138, 30]: MSE Loss: 0.0029\n",
      "[138, 45]: MSE Loss: 0.0052\n",
      "[138, 60]: MSE Loss: 0.0028\n",
      "Epoch 139\n",
      "[139, 15]: MSE Loss: 0.0032\n",
      "[139, 30]: MSE Loss: 0.0050\n",
      "[139, 45]: MSE Loss: 0.0039\n",
      "[139, 60]: MSE Loss: 0.0053\n",
      "Epoch 140\n",
      "[140, 15]: MSE Loss: 0.0048\n",
      "[140, 30]: MSE Loss: 0.0013\n",
      "[140, 45]: MSE Loss: 0.0022\n",
      "[140, 60]: MSE Loss: 0.0023\n",
      "Test Loss: 0.07166\n",
      "########################################\n",
      "Epoch 141\n",
      "[141, 15]: MSE Loss: 0.0057\n",
      "[141, 30]: MSE Loss: 0.0041\n",
      "[141, 45]: MSE Loss: 0.0029\n",
      "[141, 60]: MSE Loss: 0.0025\n",
      "Epoch 142\n",
      "[142, 15]: MSE Loss: 0.0029\n",
      "[142, 30]: MSE Loss: 0.0016\n",
      "[142, 45]: MSE Loss: 0.0016\n",
      "[142, 60]: MSE Loss: 0.0035\n",
      "Epoch 143\n",
      "[143, 15]: MSE Loss: 0.0030\n",
      "[143, 30]: MSE Loss: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 45]: MSE Loss: 0.0041\n",
      "[143, 60]: MSE Loss: 0.0026\n",
      "Epoch 144\n",
      "[144, 15]: MSE Loss: 0.0050\n",
      "[144, 30]: MSE Loss: 0.0026\n",
      "[144, 45]: MSE Loss: 0.0087\n",
      "[144, 60]: MSE Loss: 0.0041\n",
      "Epoch 145\n",
      "[145, 15]: MSE Loss: 0.0023\n",
      "[145, 30]: MSE Loss: 0.0040\n",
      "[145, 45]: MSE Loss: 0.0063\n",
      "[145, 60]: MSE Loss: 0.0032\n",
      "Epoch 146\n",
      "[146, 15]: MSE Loss: 0.0046\n",
      "[146, 30]: MSE Loss: 0.0032\n",
      "[146, 45]: MSE Loss: 0.0015\n",
      "[146, 60]: MSE Loss: 0.0031\n",
      "Epoch 147\n",
      "[147, 15]: MSE Loss: 0.0016\n",
      "[147, 30]: MSE Loss: 0.0038\n",
      "[147, 45]: MSE Loss: 0.0023\n",
      "[147, 60]: MSE Loss: 0.0029\n",
      "Epoch 148\n",
      "[148, 15]: MSE Loss: 0.0043\n",
      "[148, 30]: MSE Loss: 0.0030\n",
      "[148, 45]: MSE Loss: 0.0066\n",
      "[148, 60]: MSE Loss: 0.0113\n",
      "Epoch 149\n",
      "[149, 15]: MSE Loss: 0.0185\n",
      "[149, 30]: MSE Loss: 0.0121\n",
      "[149, 45]: MSE Loss: 0.0055\n",
      "[149, 60]: MSE Loss: 0.0064\n",
      "Epoch 150\n",
      "[150, 15]: MSE Loss: 0.0035\n",
      "[150, 30]: MSE Loss: 0.0034\n",
      "[150, 45]: MSE Loss: 0.0099\n",
      "[150, 60]: MSE Loss: 0.0040\n",
      "Test Loss: 0.17478\n",
      "########################################\n",
      "Epoch 151\n",
      "[151, 15]: MSE Loss: 0.0043\n",
      "[151, 30]: MSE Loss: 0.0037\n",
      "[151, 45]: MSE Loss: 0.0061\n",
      "[151, 60]: MSE Loss: 0.0032\n",
      "Epoch 152\n",
      "[152, 15]: MSE Loss: 0.0046\n",
      "[152, 30]: MSE Loss: 0.0028\n",
      "[152, 45]: MSE Loss: 0.0031\n",
      "[152, 60]: MSE Loss: 0.0030\n",
      "Epoch 153\n",
      "[153, 15]: MSE Loss: 0.0060\n",
      "[153, 30]: MSE Loss: 0.0041\n",
      "[153, 45]: MSE Loss: 0.0036\n",
      "[153, 60]: MSE Loss: 0.0035\n",
      "Epoch 154\n",
      "[154, 15]: MSE Loss: 0.0022\n",
      "[154, 30]: MSE Loss: 0.0026\n",
      "[154, 45]: MSE Loss: 0.0020\n",
      "[154, 60]: MSE Loss: 0.0030\n",
      "Epoch 155\n",
      "[155, 15]: MSE Loss: 0.0030\n",
      "[155, 30]: MSE Loss: 0.0028\n",
      "[155, 45]: MSE Loss: 0.0039\n",
      "[155, 60]: MSE Loss: 0.0031\n",
      "Epoch 156\n",
      "[156, 15]: MSE Loss: 0.0020\n",
      "[156, 30]: MSE Loss: 0.0027\n",
      "[156, 45]: MSE Loss: 0.0035\n",
      "[156, 60]: MSE Loss: 0.0031\n",
      "Epoch 157\n",
      "[157, 15]: MSE Loss: 0.0026\n",
      "[157, 30]: MSE Loss: 0.0025\n",
      "[157, 45]: MSE Loss: 0.0020\n",
      "[157, 60]: MSE Loss: 0.0017\n",
      "Epoch 158\n",
      "[158, 15]: MSE Loss: 0.0023\n",
      "[158, 30]: MSE Loss: 0.0046\n",
      "[158, 45]: MSE Loss: 0.0042\n",
      "[158, 60]: MSE Loss: 0.0029\n",
      "Epoch 159\n",
      "[159, 15]: MSE Loss: 0.0019\n",
      "[159, 30]: MSE Loss: 0.0043\n",
      "[159, 45]: MSE Loss: 0.0046\n",
      "[159, 60]: MSE Loss: 0.0031\n",
      "Epoch 160\n",
      "[160, 15]: MSE Loss: 0.0038\n",
      "[160, 30]: MSE Loss: 0.0038\n",
      "[160, 45]: MSE Loss: 0.0026\n",
      "[160, 60]: MSE Loss: 0.0028\n",
      "Test Loss: 0.16290\n",
      "########################################\n",
      "Epoch 161\n",
      "[161, 15]: MSE Loss: 0.0029\n",
      "[161, 30]: MSE Loss: 0.0018\n",
      "[161, 45]: MSE Loss: 0.0023\n",
      "[161, 60]: MSE Loss: 0.0022\n",
      "Epoch 162\n",
      "[162, 15]: MSE Loss: 0.0025\n",
      "[162, 30]: MSE Loss: 0.0021\n",
      "[162, 45]: MSE Loss: 0.0015\n",
      "[162, 60]: MSE Loss: 0.0021\n",
      "Epoch 163\n",
      "[163, 15]: MSE Loss: 0.0040\n",
      "[163, 30]: MSE Loss: 0.0035\n",
      "[163, 45]: MSE Loss: 0.0028\n",
      "[163, 60]: MSE Loss: 0.0021\n",
      "Epoch 164\n",
      "[164, 15]: MSE Loss: 0.0035\n",
      "[164, 30]: MSE Loss: 0.0021\n",
      "[164, 45]: MSE Loss: 0.0037\n",
      "[164, 60]: MSE Loss: 0.0019\n",
      "Epoch 165\n",
      "[165, 15]: MSE Loss: 0.0035\n",
      "[165, 30]: MSE Loss: 0.0034\n",
      "[165, 45]: MSE Loss: 0.0026\n",
      "[165, 60]: MSE Loss: 0.0028\n",
      "Epoch 166\n",
      "[166, 15]: MSE Loss: 0.0058\n",
      "[166, 30]: MSE Loss: 0.0043\n",
      "[166, 45]: MSE Loss: 0.0044\n",
      "[166, 60]: MSE Loss: 0.0023\n",
      "Epoch 167\n",
      "[167, 15]: MSE Loss: 0.0021\n",
      "[167, 30]: MSE Loss: 0.0018\n",
      "[167, 45]: MSE Loss: 0.0035\n",
      "[167, 60]: MSE Loss: 0.0035\n",
      "Epoch 168\n",
      "[168, 15]: MSE Loss: 0.0020\n",
      "[168, 30]: MSE Loss: 0.0021\n",
      "[168, 45]: MSE Loss: 0.0019\n",
      "[168, 60]: MSE Loss: 0.0027\n",
      "Epoch 169\n",
      "[169, 15]: MSE Loss: 0.0024\n",
      "[169, 30]: MSE Loss: 0.0023\n",
      "[169, 45]: MSE Loss: 0.0021\n",
      "[169, 60]: MSE Loss: 0.0024\n",
      "Epoch 170\n",
      "[170, 15]: MSE Loss: 0.0021\n",
      "[170, 30]: MSE Loss: 0.0027\n",
      "[170, 45]: MSE Loss: 0.0019\n",
      "[170, 60]: MSE Loss: 0.0049\n",
      "Test Loss: 0.18457\n",
      "########################################\n",
      "Epoch 171\n",
      "[171, 15]: MSE Loss: 0.0048\n",
      "[171, 30]: MSE Loss: 0.0036\n",
      "[171, 45]: MSE Loss: 0.0027\n",
      "[171, 60]: MSE Loss: 0.0019\n",
      "Epoch 172\n",
      "[172, 15]: MSE Loss: 0.0086\n",
      "[172, 30]: MSE Loss: 0.0031\n",
      "[172, 45]: MSE Loss: 0.0019\n",
      "[172, 60]: MSE Loss: 0.0035\n",
      "Epoch 173\n",
      "[173, 15]: MSE Loss: 0.0063\n",
      "[173, 30]: MSE Loss: 0.0036\n",
      "[173, 45]: MSE Loss: 0.0060\n",
      "[173, 60]: MSE Loss: 0.0083\n",
      "Epoch 174\n",
      "[174, 15]: MSE Loss: 0.0031\n",
      "[174, 30]: MSE Loss: 0.0040\n",
      "[174, 45]: MSE Loss: 0.0027\n",
      "[174, 60]: MSE Loss: 0.0037\n",
      "Epoch 175\n",
      "[175, 15]: MSE Loss: 0.0046\n",
      "[175, 30]: MSE Loss: 0.0027\n",
      "[175, 45]: MSE Loss: 0.0035\n",
      "[175, 60]: MSE Loss: 0.0030\n",
      "Epoch 176\n",
      "[176, 15]: MSE Loss: 0.0026\n",
      "[176, 30]: MSE Loss: 0.0042\n",
      "[176, 45]: MSE Loss: 0.0041\n",
      "[176, 60]: MSE Loss: 0.0056\n",
      "Epoch 177\n",
      "[177, 15]: MSE Loss: 0.0078\n",
      "[177, 30]: MSE Loss: 0.0041\n",
      "[177, 45]: MSE Loss: 0.0057\n",
      "[177, 60]: MSE Loss: 0.0030\n",
      "Epoch 178\n",
      "[178, 15]: MSE Loss: 0.0024\n",
      "[178, 30]: MSE Loss: 0.0028\n",
      "[178, 45]: MSE Loss: 0.0017\n",
      "[178, 60]: MSE Loss: 0.0032\n",
      "Epoch 179\n",
      "[179, 15]: MSE Loss: 0.0019\n",
      "[179, 30]: MSE Loss: 0.0023\n",
      "[179, 45]: MSE Loss: 0.0019\n",
      "[179, 60]: MSE Loss: 0.0036\n",
      "Epoch 180\n",
      "[180, 15]: MSE Loss: 0.0034\n",
      "[180, 30]: MSE Loss: 0.0029\n",
      "[180, 45]: MSE Loss: 0.0036\n",
      "[180, 60]: MSE Loss: 0.0019\n",
      "Test Loss: 0.04745\n",
      "########################################\n",
      "Epoch 181\n",
      "[181, 15]: MSE Loss: 0.0073\n",
      "[181, 30]: MSE Loss: 0.0038\n",
      "[181, 45]: MSE Loss: 0.0038\n",
      "[181, 60]: MSE Loss: 0.0058\n",
      "Epoch 182\n",
      "[182, 15]: MSE Loss: 0.0052\n",
      "[182, 30]: MSE Loss: 0.0041\n",
      "[182, 45]: MSE Loss: 0.0083\n",
      "[182, 60]: MSE Loss: 0.0039\n",
      "Epoch 183\n",
      "[183, 15]: MSE Loss: 0.0035\n",
      "[183, 30]: MSE Loss: 0.0029\n",
      "[183, 45]: MSE Loss: 0.0029\n",
      "[183, 60]: MSE Loss: 0.0065\n",
      "Epoch 184\n",
      "[184, 15]: MSE Loss: 0.0039\n",
      "[184, 30]: MSE Loss: 0.0025\n",
      "[184, 45]: MSE Loss: 0.0030\n",
      "[184, 60]: MSE Loss: 0.0032\n",
      "Epoch 185\n",
      "[185, 15]: MSE Loss: 0.0020\n",
      "[185, 30]: MSE Loss: 0.0029\n",
      "[185, 45]: MSE Loss: 0.0019\n",
      "[185, 60]: MSE Loss: 0.0030\n",
      "Epoch 186\n",
      "[186, 15]: MSE Loss: 0.0025\n",
      "[186, 30]: MSE Loss: 0.0043\n",
      "[186, 45]: MSE Loss: 0.0026\n",
      "[186, 60]: MSE Loss: 0.0025\n",
      "Epoch 187\n",
      "[187, 15]: MSE Loss: 0.0028\n",
      "[187, 30]: MSE Loss: 0.0026\n",
      "[187, 45]: MSE Loss: 0.0028\n",
      "[187, 60]: MSE Loss: 0.0028\n",
      "Epoch 188\n",
      "[188, 15]: MSE Loss: 0.0018\n",
      "[188, 30]: MSE Loss: 0.0030\n",
      "[188, 45]: MSE Loss: 0.0030\n",
      "[188, 60]: MSE Loss: 0.0020\n",
      "Epoch 189\n",
      "[189, 15]: MSE Loss: 0.0018\n",
      "[189, 30]: MSE Loss: 0.0017\n",
      "[189, 45]: MSE Loss: 0.0058\n",
      "[189, 60]: MSE Loss: 0.0019\n",
      "Epoch 190\n",
      "[190, 15]: MSE Loss: 0.0040\n",
      "[190, 30]: MSE Loss: 0.0016\n",
      "[190, 45]: MSE Loss: 0.0029\n",
      "[190, 60]: MSE Loss: 0.0024\n",
      "Test Loss: 0.09250\n",
      "########################################\n",
      "Epoch 191\n",
      "[191, 15]: MSE Loss: 0.0023\n",
      "[191, 30]: MSE Loss: 0.0092\n",
      "[191, 45]: MSE Loss: 0.0022\n",
      "[191, 60]: MSE Loss: 0.0024\n",
      "Epoch 192\n",
      "[192, 15]: MSE Loss: 0.0042\n",
      "[192, 30]: MSE Loss: 0.0016\n",
      "[192, 45]: MSE Loss: 0.0023\n",
      "[192, 60]: MSE Loss: 0.0028\n",
      "Epoch 193\n",
      "[193, 15]: MSE Loss: 0.0036\n",
      "[193, 30]: MSE Loss: 0.0022\n",
      "[193, 45]: MSE Loss: 0.0020\n",
      "[193, 60]: MSE Loss: 0.0024\n",
      "Epoch 194\n",
      "[194, 15]: MSE Loss: 0.0026\n",
      "[194, 30]: MSE Loss: 0.0029\n",
      "[194, 45]: MSE Loss: 0.0033\n",
      "[194, 60]: MSE Loss: 0.0021\n",
      "Epoch 195\n",
      "[195, 15]: MSE Loss: 0.0016\n",
      "[195, 30]: MSE Loss: 0.0029\n",
      "[195, 45]: MSE Loss: 0.0023\n",
      "[195, 60]: MSE Loss: 0.0021\n",
      "Epoch 196\n",
      "[196, 15]: MSE Loss: 0.0047\n",
      "[196, 30]: MSE Loss: 0.0026\n",
      "[196, 45]: MSE Loss: 0.0020\n",
      "[196, 60]: MSE Loss: 0.0035\n",
      "Epoch 197\n",
      "[197, 15]: MSE Loss: 0.0019\n",
      "[197, 30]: MSE Loss: 0.0042\n",
      "[197, 45]: MSE Loss: 0.0038\n",
      "[197, 60]: MSE Loss: 0.0014\n",
      "Epoch 198\n",
      "[198, 15]: MSE Loss: 0.0034\n",
      "[198, 30]: MSE Loss: 0.0029\n",
      "[198, 45]: MSE Loss: 0.0035\n",
      "[198, 60]: MSE Loss: 0.0030\n",
      "Epoch 199\n",
      "[199, 15]: MSE Loss: 0.0023\n",
      "[199, 30]: MSE Loss: 0.0035\n",
      "[199, 45]: MSE Loss: 0.0019\n",
      "[199, 60]: MSE Loss: 0.0016\n",
      "Epoch 200\n",
      "[200, 15]: MSE Loss: 0.0021\n",
      "[200, 30]: MSE Loss: 0.0032\n",
      "[200, 45]: MSE Loss: 0.0022\n",
      "[200, 60]: MSE Loss: 0.0028\n",
      "Test Loss: 0.05128\n",
      "########################################\n",
      "Epoch 201\n",
      "[201, 15]: MSE Loss: 0.0029\n",
      "[201, 30]: MSE Loss: 0.0026\n",
      "[201, 45]: MSE Loss: 0.0024\n",
      "[201, 60]: MSE Loss: 0.0020\n",
      "Epoch 202\n",
      "[202, 15]: MSE Loss: 0.0043\n",
      "[202, 30]: MSE Loss: 0.0033\n",
      "[202, 45]: MSE Loss: 0.0014\n",
      "[202, 60]: MSE Loss: 0.0026\n",
      "Epoch 203\n",
      "[203, 15]: MSE Loss: 0.0019\n",
      "[203, 30]: MSE Loss: 0.0040\n",
      "[203, 45]: MSE Loss: 0.0034\n",
      "[203, 60]: MSE Loss: 0.0018\n",
      "Epoch 204\n",
      "[204, 15]: MSE Loss: 0.0022\n",
      "[204, 30]: MSE Loss: 0.0026\n",
      "[204, 45]: MSE Loss: 0.0022\n",
      "[204, 60]: MSE Loss: 0.0054\n",
      "Epoch 205\n",
      "[205, 15]: MSE Loss: 0.0021\n",
      "[205, 30]: MSE Loss: 0.0018\n",
      "[205, 45]: MSE Loss: 0.0022\n",
      "[205, 60]: MSE Loss: 0.0035\n",
      "Epoch 206\n",
      "[206, 15]: MSE Loss: 0.0042\n",
      "[206, 30]: MSE Loss: 0.0090\n",
      "[206, 45]: MSE Loss: 0.0059\n",
      "[206, 60]: MSE Loss: 0.0057\n",
      "Epoch 207\n",
      "[207, 15]: MSE Loss: 0.0040\n",
      "[207, 30]: MSE Loss: 0.0050\n",
      "[207, 45]: MSE Loss: 0.0039\n",
      "[207, 60]: MSE Loss: 0.0044\n",
      "Epoch 208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208, 15]: MSE Loss: 0.0031\n",
      "[208, 30]: MSE Loss: 0.0023\n",
      "[208, 45]: MSE Loss: 0.0042\n",
      "[208, 60]: MSE Loss: 0.0050\n",
      "Epoch 209\n",
      "[209, 15]: MSE Loss: 0.0092\n",
      "[209, 30]: MSE Loss: 0.0037\n",
      "[209, 45]: MSE Loss: 0.0036\n",
      "[209, 60]: MSE Loss: 0.0022\n",
      "Epoch 210\n",
      "[210, 15]: MSE Loss: 0.0025\n",
      "[210, 30]: MSE Loss: 0.0028\n",
      "[210, 45]: MSE Loss: 0.0024\n",
      "[210, 60]: MSE Loss: 0.0066\n",
      "Test Loss: 0.06577\n",
      "########################################\n",
      "Epoch 211\n",
      "[211, 15]: MSE Loss: 0.0036\n",
      "[211, 30]: MSE Loss: 0.0020\n",
      "[211, 45]: MSE Loss: 0.0034\n",
      "[211, 60]: MSE Loss: 0.0029\n",
      "Epoch 212\n",
      "[212, 15]: MSE Loss: 0.0023\n",
      "[212, 30]: MSE Loss: 0.0023\n",
      "[212, 45]: MSE Loss: 0.0033\n",
      "[212, 60]: MSE Loss: 0.0023\n",
      "Epoch 213\n",
      "[213, 15]: MSE Loss: 0.0022\n",
      "[213, 30]: MSE Loss: 0.0031\n",
      "[213, 45]: MSE Loss: 0.0040\n",
      "[213, 60]: MSE Loss: 0.0029\n",
      "Epoch 214\n",
      "[214, 15]: MSE Loss: 0.0029\n",
      "[214, 30]: MSE Loss: 0.0028\n",
      "[214, 45]: MSE Loss: 0.0018\n",
      "[214, 60]: MSE Loss: 0.0028\n",
      "Epoch 215\n",
      "[215, 15]: MSE Loss: 0.0021\n",
      "[215, 30]: MSE Loss: 0.0021\n",
      "[215, 45]: MSE Loss: 0.0024\n",
      "[215, 60]: MSE Loss: 0.0037\n",
      "Epoch 216\n",
      "[216, 15]: MSE Loss: 0.0016\n",
      "[216, 30]: MSE Loss: 0.0030\n",
      "[216, 45]: MSE Loss: 0.0100\n",
      "[216, 60]: MSE Loss: 0.0042\n",
      "Epoch 217\n",
      "[217, 15]: MSE Loss: 0.0044\n",
      "[217, 30]: MSE Loss: 0.0038\n",
      "[217, 45]: MSE Loss: 0.0024\n",
      "[217, 60]: MSE Loss: 0.0044\n",
      "Epoch 218\n",
      "[218, 15]: MSE Loss: 0.0042\n",
      "[218, 30]: MSE Loss: 0.0029\n",
      "[218, 45]: MSE Loss: 0.0020\n",
      "[218, 60]: MSE Loss: 0.0044\n",
      "Epoch 219\n",
      "[219, 15]: MSE Loss: 0.0019\n",
      "[219, 30]: MSE Loss: 0.0016\n",
      "[219, 45]: MSE Loss: 0.0046\n",
      "[219, 60]: MSE Loss: 0.0023\n",
      "Epoch 220\n",
      "[220, 15]: MSE Loss: 0.0018\n",
      "[220, 30]: MSE Loss: 0.0044\n",
      "[220, 45]: MSE Loss: 0.0026\n",
      "[220, 60]: MSE Loss: 0.0044\n",
      "Test Loss: 0.05382\n",
      "########################################\n",
      "Epoch 221\n",
      "[221, 15]: MSE Loss: 0.0032\n",
      "[221, 30]: MSE Loss: 0.0027\n",
      "[221, 45]: MSE Loss: 0.0035\n",
      "[221, 60]: MSE Loss: 0.0030\n",
      "Epoch 222\n",
      "[222, 15]: MSE Loss: 0.0014\n",
      "[222, 30]: MSE Loss: 0.0028\n",
      "[222, 45]: MSE Loss: 0.0020\n",
      "[222, 60]: MSE Loss: 0.0026\n",
      "Epoch 223\n",
      "[223, 15]: MSE Loss: 0.0015\n",
      "[223, 30]: MSE Loss: 0.0016\n",
      "[223, 45]: MSE Loss: 0.0025\n",
      "[223, 60]: MSE Loss: 0.0039\n",
      "Epoch 224\n",
      "[224, 15]: MSE Loss: 0.0038\n",
      "[224, 30]: MSE Loss: 0.0029\n",
      "[224, 45]: MSE Loss: 0.0026\n",
      "[224, 60]: MSE Loss: 0.0024\n",
      "Epoch 225\n",
      "[225, 15]: MSE Loss: 0.0017\n",
      "[225, 30]: MSE Loss: 0.0016\n",
      "[225, 45]: MSE Loss: 0.0019\n",
      "[225, 60]: MSE Loss: 0.0029\n",
      "Epoch 226\n",
      "[226, 15]: MSE Loss: 0.0016\n",
      "[226, 30]: MSE Loss: 0.0037\n",
      "[226, 45]: MSE Loss: 0.0019\n",
      "[226, 60]: MSE Loss: 0.0018\n",
      "Epoch 227\n",
      "[227, 15]: MSE Loss: 0.0023\n",
      "[227, 30]: MSE Loss: 0.0018\n",
      "[227, 45]: MSE Loss: 0.0021\n",
      "[227, 60]: MSE Loss: 0.0018\n",
      "Epoch 228\n",
      "[228, 15]: MSE Loss: 0.0021\n",
      "[228, 30]: MSE Loss: 0.0022\n",
      "[228, 45]: MSE Loss: 0.0020\n",
      "[228, 60]: MSE Loss: 0.0107\n",
      "Epoch 229\n",
      "[229, 15]: MSE Loss: 0.0017\n",
      "[229, 30]: MSE Loss: 0.0021\n",
      "[229, 45]: MSE Loss: 0.0017\n",
      "[229, 60]: MSE Loss: 0.0028\n",
      "Epoch 230\n",
      "[230, 15]: MSE Loss: 0.0021\n",
      "[230, 30]: MSE Loss: 0.0013\n",
      "[230, 45]: MSE Loss: 0.0021\n",
      "[230, 60]: MSE Loss: 0.0025\n",
      "Test Loss: 0.04922\n",
      "########################################\n",
      "Epoch 231\n",
      "[231, 15]: MSE Loss: 0.0028\n",
      "[231, 30]: MSE Loss: 0.0014\n",
      "[231, 45]: MSE Loss: 0.0013\n",
      "[231, 60]: MSE Loss: 0.0020\n",
      "Epoch 232\n",
      "[232, 15]: MSE Loss: 0.0017\n",
      "[232, 30]: MSE Loss: 0.0023\n",
      "[232, 45]: MSE Loss: 0.0018\n",
      "[232, 60]: MSE Loss: 0.0018\n",
      "Epoch 233\n",
      "[233, 15]: MSE Loss: 0.0026\n",
      "[233, 30]: MSE Loss: 0.0026\n",
      "[233, 45]: MSE Loss: 0.0025\n",
      "[233, 60]: MSE Loss: 0.0018\n",
      "Epoch 234\n",
      "[234, 15]: MSE Loss: 0.0030\n",
      "[234, 30]: MSE Loss: 0.0039\n",
      "[234, 45]: MSE Loss: 0.0033\n",
      "[234, 60]: MSE Loss: 0.0037\n",
      "Epoch 235\n",
      "[235, 15]: MSE Loss: 0.0016\n",
      "[235, 30]: MSE Loss: 0.0022\n",
      "[235, 45]: MSE Loss: 0.0028\n",
      "[235, 60]: MSE Loss: 0.0017\n",
      "Epoch 236\n",
      "[236, 15]: MSE Loss: 0.0014\n",
      "[236, 30]: MSE Loss: 0.0022\n",
      "[236, 45]: MSE Loss: 0.0024\n",
      "[236, 60]: MSE Loss: 0.0012\n",
      "Epoch 237\n",
      "[237, 15]: MSE Loss: 0.0009\n",
      "[237, 30]: MSE Loss: 0.0025\n",
      "[237, 45]: MSE Loss: 0.0023\n",
      "[237, 60]: MSE Loss: 0.0031\n",
      "Epoch 238\n",
      "[238, 15]: MSE Loss: 0.0020\n",
      "[238, 30]: MSE Loss: 0.0027\n",
      "[238, 45]: MSE Loss: 0.0022\n",
      "[238, 60]: MSE Loss: 0.0025\n",
      "Epoch 239\n",
      "[239, 15]: MSE Loss: 0.0016\n",
      "[239, 30]: MSE Loss: 0.0021\n",
      "[239, 45]: MSE Loss: 0.0018\n",
      "[239, 60]: MSE Loss: 0.0019\n",
      "Epoch 240\n",
      "[240, 15]: MSE Loss: 0.0016\n",
      "[240, 30]: MSE Loss: 0.0023\n",
      "[240, 45]: MSE Loss: 0.0017\n",
      "[240, 60]: MSE Loss: 0.0027\n",
      "Test Loss: 0.04756\n",
      "########################################\n",
      "Epoch 241\n",
      "[241, 15]: MSE Loss: 0.0009\n",
      "[241, 30]: MSE Loss: 0.0023\n",
      "[241, 45]: MSE Loss: 0.0032\n",
      "[241, 60]: MSE Loss: 0.0021\n",
      "Epoch 242\n",
      "[242, 15]: MSE Loss: 0.0022\n",
      "[242, 30]: MSE Loss: 0.0041\n",
      "[242, 45]: MSE Loss: 0.0028\n",
      "[242, 60]: MSE Loss: 0.0024\n",
      "Epoch 243\n",
      "[243, 15]: MSE Loss: 0.0038\n",
      "[243, 30]: MSE Loss: 0.0015\n",
      "[243, 45]: MSE Loss: 0.0035\n",
      "[243, 60]: MSE Loss: 0.0024\n",
      "Epoch 244\n",
      "[244, 15]: MSE Loss: 0.0012\n",
      "[244, 30]: MSE Loss: 0.0038\n",
      "[244, 45]: MSE Loss: 0.0014\n",
      "[244, 60]: MSE Loss: 0.0023\n",
      "Epoch 245\n",
      "[245, 15]: MSE Loss: 0.0038\n",
      "[245, 30]: MSE Loss: 0.0052\n",
      "[245, 45]: MSE Loss: 0.0061\n",
      "[245, 60]: MSE Loss: 0.0016\n",
      "Epoch 246\n",
      "[246, 15]: MSE Loss: 0.0025\n",
      "[246, 30]: MSE Loss: 0.0017\n",
      "[246, 45]: MSE Loss: 0.0032\n",
      "[246, 60]: MSE Loss: 0.0018\n",
      "Epoch 247\n",
      "[247, 15]: MSE Loss: 0.0015\n",
      "[247, 30]: MSE Loss: 0.0016\n",
      "[247, 45]: MSE Loss: 0.0020\n",
      "[247, 60]: MSE Loss: 0.0015\n",
      "Epoch 248\n",
      "[248, 15]: MSE Loss: 0.0023\n",
      "[248, 30]: MSE Loss: 0.0021\n",
      "[248, 45]: MSE Loss: 0.0011\n",
      "[248, 60]: MSE Loss: 0.0017\n",
      "Epoch 249\n",
      "[249, 15]: MSE Loss: 0.0032\n",
      "[249, 30]: MSE Loss: 0.0010\n",
      "[249, 45]: MSE Loss: 0.0018\n",
      "[249, 60]: MSE Loss: 0.0026\n",
      "Epoch 250\n",
      "[250, 15]: MSE Loss: 0.0018\n",
      "[250, 30]: MSE Loss: 0.0038\n",
      "[250, 45]: MSE Loss: 0.0027\n",
      "[250, 60]: MSE Loss: 0.0015\n",
      "Test Loss: 0.16724\n",
      "########################################\n",
      "Epoch 251\n",
      "[251, 15]: MSE Loss: 0.0038\n",
      "[251, 30]: MSE Loss: 0.0016\n",
      "[251, 45]: MSE Loss: 0.0016\n",
      "[251, 60]: MSE Loss: 0.0021\n",
      "Epoch 252\n",
      "[252, 15]: MSE Loss: 0.0023\n",
      "[252, 30]: MSE Loss: 0.0012\n",
      "[252, 45]: MSE Loss: 0.0022\n",
      "[252, 60]: MSE Loss: 0.0019\n",
      "Epoch 253\n",
      "[253, 15]: MSE Loss: 0.0018\n",
      "[253, 30]: MSE Loss: 0.0022\n",
      "[253, 45]: MSE Loss: 0.0051\n",
      "[253, 60]: MSE Loss: 0.0021\n",
      "Epoch 254\n",
      "[254, 15]: MSE Loss: 0.0014\n",
      "[254, 30]: MSE Loss: 0.0017\n",
      "[254, 45]: MSE Loss: 0.0038\n",
      "[254, 60]: MSE Loss: 0.0012\n",
      "Epoch 255\n",
      "[255, 15]: MSE Loss: 0.0019\n",
      "[255, 30]: MSE Loss: 0.0010\n",
      "[255, 45]: MSE Loss: 0.0023\n",
      "[255, 60]: MSE Loss: 0.0017\n",
      "Epoch 256\n",
      "[256, 15]: MSE Loss: 0.0018\n",
      "[256, 30]: MSE Loss: 0.0020\n",
      "[256, 45]: MSE Loss: 0.0011\n",
      "[256, 60]: MSE Loss: 0.0016\n",
      "Epoch 257\n",
      "[257, 15]: MSE Loss: 0.0045\n",
      "[257, 30]: MSE Loss: 0.0021\n",
      "[257, 45]: MSE Loss: 0.0011\n",
      "[257, 60]: MSE Loss: 0.0016\n",
      "Epoch 258\n",
      "[258, 15]: MSE Loss: 0.0019\n",
      "[258, 30]: MSE Loss: 0.0018\n",
      "[258, 45]: MSE Loss: 0.0025\n",
      "[258, 60]: MSE Loss: 0.0017\n",
      "Epoch 259\n",
      "[259, 15]: MSE Loss: 0.0025\n",
      "[259, 30]: MSE Loss: 0.0015\n",
      "[259, 45]: MSE Loss: 0.0016\n",
      "[259, 60]: MSE Loss: 0.0046\n",
      "Epoch 260\n",
      "[260, 15]: MSE Loss: 0.0027\n",
      "[260, 30]: MSE Loss: 0.0038\n",
      "[260, 45]: MSE Loss: 0.0019\n",
      "[260, 60]: MSE Loss: 0.0014\n",
      "Test Loss: 0.06154\n",
      "########################################\n",
      "Epoch 261\n",
      "[261, 15]: MSE Loss: 0.0023\n",
      "[261, 30]: MSE Loss: 0.0017\n",
      "[261, 45]: MSE Loss: 0.0023\n",
      "[261, 60]: MSE Loss: 0.0015\n",
      "Epoch 262\n",
      "[262, 15]: MSE Loss: 0.0048\n",
      "[262, 30]: MSE Loss: 0.0020\n",
      "[262, 45]: MSE Loss: 0.0016\n",
      "[262, 60]: MSE Loss: 0.0022\n",
      "Epoch 263\n",
      "[263, 15]: MSE Loss: 0.0027\n",
      "[263, 30]: MSE Loss: 0.0015\n",
      "[263, 45]: MSE Loss: 0.0027\n",
      "[263, 60]: MSE Loss: 0.0032\n",
      "Epoch 264\n",
      "[264, 15]: MSE Loss: 0.0018\n",
      "[264, 30]: MSE Loss: 0.0013\n",
      "[264, 45]: MSE Loss: 0.0026\n",
      "[264, 60]: MSE Loss: 0.0023\n",
      "Epoch 265\n",
      "[265, 15]: MSE Loss: 0.0053\n",
      "[265, 30]: MSE Loss: 0.0017\n",
      "[265, 45]: MSE Loss: 0.0070\n",
      "[265, 60]: MSE Loss: 0.0021\n",
      "Epoch 266\n",
      "[266, 15]: MSE Loss: 0.0022\n",
      "[266, 30]: MSE Loss: 0.0017\n",
      "[266, 45]: MSE Loss: 0.0042\n",
      "[266, 60]: MSE Loss: 0.0017\n",
      "Epoch 267\n",
      "[267, 15]: MSE Loss: 0.0027\n",
      "[267, 30]: MSE Loss: 0.0022\n",
      "[267, 45]: MSE Loss: 0.0032\n",
      "[267, 60]: MSE Loss: 0.0018\n",
      "Epoch 268\n",
      "[268, 15]: MSE Loss: 0.0016\n",
      "[268, 30]: MSE Loss: 0.0030\n",
      "[268, 45]: MSE Loss: 0.0025\n",
      "[268, 60]: MSE Loss: 0.0014\n",
      "Epoch 269\n",
      "[269, 15]: MSE Loss: 0.0008\n",
      "[269, 30]: MSE Loss: 0.0011\n",
      "[269, 45]: MSE Loss: 0.0017\n",
      "[269, 60]: MSE Loss: 0.0015\n",
      "Epoch 270\n",
      "[270, 15]: MSE Loss: 0.0020\n",
      "[270, 30]: MSE Loss: 0.0010\n",
      "[270, 45]: MSE Loss: 0.0012\n",
      "[270, 60]: MSE Loss: 0.0013\n",
      "Test Loss: 0.15105\n",
      "########################################\n",
      "Epoch 271\n",
      "[271, 15]: MSE Loss: 0.0012\n",
      "[271, 30]: MSE Loss: 0.0017\n",
      "[271, 45]: MSE Loss: 0.0021\n",
      "[271, 60]: MSE Loss: 0.0016\n",
      "Epoch 272\n",
      "[272, 15]: MSE Loss: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272, 30]: MSE Loss: 0.0015\n",
      "[272, 45]: MSE Loss: 0.0023\n",
      "[272, 60]: MSE Loss: 0.0031\n",
      "Epoch 273\n",
      "[273, 15]: MSE Loss: 0.0021\n",
      "[273, 30]: MSE Loss: 0.0011\n",
      "[273, 45]: MSE Loss: 0.0016\n",
      "[273, 60]: MSE Loss: 0.0027\n",
      "Epoch 274\n",
      "[274, 15]: MSE Loss: 0.0020\n",
      "[274, 30]: MSE Loss: 0.0024\n",
      "[274, 45]: MSE Loss: 0.0012\n",
      "[274, 60]: MSE Loss: 0.0023\n",
      "Epoch 275\n",
      "[275, 15]: MSE Loss: 0.0019\n",
      "[275, 30]: MSE Loss: 0.0041\n",
      "[275, 45]: MSE Loss: 0.0029\n",
      "[275, 60]: MSE Loss: 0.0014\n",
      "Epoch 276\n",
      "[276, 15]: MSE Loss: 0.0056\n",
      "[276, 30]: MSE Loss: 0.0014\n",
      "[276, 45]: MSE Loss: 0.0022\n",
      "[276, 60]: MSE Loss: 0.0015\n",
      "Epoch 277\n",
      "[277, 15]: MSE Loss: 0.0031\n",
      "[277, 30]: MSE Loss: 0.0023\n",
      "[277, 45]: MSE Loss: 0.0035\n",
      "[277, 60]: MSE Loss: 0.0021\n",
      "Epoch 278\n",
      "[278, 15]: MSE Loss: 0.0025\n",
      "[278, 30]: MSE Loss: 0.0018\n",
      "[278, 45]: MSE Loss: 0.0027\n",
      "[278, 60]: MSE Loss: 0.0007\n",
      "Epoch 279\n",
      "[279, 15]: MSE Loss: 0.0012\n",
      "[279, 30]: MSE Loss: 0.0019\n",
      "[279, 45]: MSE Loss: 0.0021\n",
      "[279, 60]: MSE Loss: 0.0016\n",
      "Epoch 280\n",
      "[280, 15]: MSE Loss: 0.0019\n",
      "[280, 30]: MSE Loss: 0.0068\n",
      "[280, 45]: MSE Loss: 0.0018\n",
      "[280, 60]: MSE Loss: 0.0016\n",
      "Test Loss: 0.04891\n",
      "########################################\n",
      "Epoch 281\n",
      "[281, 15]: MSE Loss: 0.0012\n",
      "[281, 30]: MSE Loss: 0.0035\n",
      "[281, 45]: MSE Loss: 0.0022\n",
      "[281, 60]: MSE Loss: 0.0014\n",
      "Epoch 282\n",
      "[282, 15]: MSE Loss: 0.0021\n",
      "[282, 30]: MSE Loss: 0.0019\n",
      "[282, 45]: MSE Loss: 0.0009\n",
      "[282, 60]: MSE Loss: 0.0022\n",
      "Epoch 283\n",
      "[283, 15]: MSE Loss: 0.0034\n",
      "[283, 30]: MSE Loss: 0.0022\n",
      "[283, 45]: MSE Loss: 0.0016\n",
      "[283, 60]: MSE Loss: 0.0017\n",
      "Epoch 284\n",
      "[284, 15]: MSE Loss: 0.0015\n",
      "[284, 30]: MSE Loss: 0.0015\n",
      "[284, 45]: MSE Loss: 0.0016\n",
      "[284, 60]: MSE Loss: 0.0013\n",
      "Epoch 285\n",
      "[285, 15]: MSE Loss: 0.0011\n",
      "[285, 30]: MSE Loss: 0.0014\n",
      "[285, 45]: MSE Loss: 0.0014\n",
      "[285, 60]: MSE Loss: 0.0018\n",
      "Epoch 286\n",
      "[286, 15]: MSE Loss: 0.0018\n",
      "[286, 30]: MSE Loss: 0.0027\n",
      "[286, 45]: MSE Loss: 0.0013\n",
      "[286, 60]: MSE Loss: 0.0011\n",
      "Epoch 287\n",
      "[287, 15]: MSE Loss: 0.0017\n",
      "[287, 30]: MSE Loss: 0.0020\n",
      "[287, 45]: MSE Loss: 0.0012\n",
      "[287, 60]: MSE Loss: 0.0029\n",
      "Epoch 288\n",
      "[288, 15]: MSE Loss: 0.0022\n",
      "[288, 30]: MSE Loss: 0.0018\n",
      "[288, 45]: MSE Loss: 0.0032\n",
      "[288, 60]: MSE Loss: 0.0020\n",
      "Epoch 289\n",
      "[289, 15]: MSE Loss: 0.0009\n",
      "[289, 30]: MSE Loss: 0.0022\n",
      "[289, 45]: MSE Loss: 0.0018\n",
      "[289, 60]: MSE Loss: 0.0016\n",
      "Epoch 290\n",
      "[290, 15]: MSE Loss: 0.0021\n",
      "[290, 30]: MSE Loss: 0.0013\n",
      "[290, 45]: MSE Loss: 0.0018\n",
      "[290, 60]: MSE Loss: 0.0028\n",
      "Test Loss: 0.04733\n",
      "########################################\n",
      "Epoch 291\n",
      "[291, 15]: MSE Loss: 0.0016\n",
      "[291, 30]: MSE Loss: 0.0020\n",
      "[291, 45]: MSE Loss: 0.0016\n",
      "[291, 60]: MSE Loss: 0.0015\n",
      "Epoch 292\n",
      "[292, 15]: MSE Loss: 0.0022\n",
      "[292, 30]: MSE Loss: 0.0015\n",
      "[292, 45]: MSE Loss: 0.0016\n",
      "[292, 60]: MSE Loss: 0.0012\n",
      "Epoch 293\n",
      "[293, 15]: MSE Loss: 0.0029\n",
      "[293, 30]: MSE Loss: 0.0016\n",
      "[293, 45]: MSE Loss: 0.0015\n",
      "[293, 60]: MSE Loss: 0.0010\n",
      "Epoch 294\n",
      "[294, 15]: MSE Loss: 0.0016\n",
      "[294, 30]: MSE Loss: 0.0019\n",
      "[294, 45]: MSE Loss: 0.0015\n",
      "[294, 60]: MSE Loss: 0.0026\n",
      "Epoch 295\n",
      "[295, 15]: MSE Loss: 0.0022\n",
      "[295, 30]: MSE Loss: 0.0013\n",
      "[295, 45]: MSE Loss: 0.0024\n",
      "[295, 60]: MSE Loss: 0.0029\n",
      "Epoch 296\n",
      "[296, 15]: MSE Loss: 0.0015\n",
      "[296, 30]: MSE Loss: 0.0029\n",
      "[296, 45]: MSE Loss: 0.0019\n",
      "[296, 60]: MSE Loss: 0.0020\n",
      "Epoch 297\n",
      "[297, 15]: MSE Loss: 0.0020\n",
      "[297, 30]: MSE Loss: 0.0029\n",
      "[297, 45]: MSE Loss: 0.0024\n",
      "[297, 60]: MSE Loss: 0.0012\n",
      "Epoch 298\n",
      "[298, 15]: MSE Loss: 0.0011\n",
      "[298, 30]: MSE Loss: 0.0016\n",
      "[298, 45]: MSE Loss: 0.0014\n",
      "[298, 60]: MSE Loss: 0.0014\n",
      "Epoch 299\n",
      "[299, 15]: MSE Loss: 0.0028\n",
      "[299, 30]: MSE Loss: 0.0010\n",
      "[299, 45]: MSE Loss: 0.0018\n",
      "[299, 60]: MSE Loss: 0.0025\n",
      "Epoch 300\n",
      "[300, 15]: MSE Loss: 0.0012\n",
      "[300, 30]: MSE Loss: 0.0020\n",
      "[300, 45]: MSE Loss: 0.0024\n",
      "[300, 60]: MSE Loss: 0.0014\n",
      "Test Loss: 0.05603\n",
      "########################################\n",
      "Epoch 301\n",
      "[301, 15]: MSE Loss: 0.0014\n",
      "[301, 30]: MSE Loss: 0.0021\n",
      "[301, 45]: MSE Loss: 0.0013\n",
      "[301, 60]: MSE Loss: 0.0015\n",
      "Epoch 302\n",
      "[302, 15]: MSE Loss: 0.0033\n",
      "[302, 30]: MSE Loss: 0.0037\n",
      "[302, 45]: MSE Loss: 0.0025\n",
      "[302, 60]: MSE Loss: 0.0036\n",
      "Epoch 303\n",
      "[303, 15]: MSE Loss: 0.0048\n",
      "[303, 30]: MSE Loss: 0.0028\n",
      "[303, 45]: MSE Loss: 0.0039\n",
      "[303, 60]: MSE Loss: 0.0028\n",
      "Epoch 304\n",
      "[304, 15]: MSE Loss: 0.0049\n",
      "[304, 30]: MSE Loss: 0.0035\n",
      "[304, 45]: MSE Loss: 0.0020\n",
      "[304, 60]: MSE Loss: 0.0030\n",
      "Epoch 305\n",
      "[305, 15]: MSE Loss: 0.0017\n",
      "[305, 30]: MSE Loss: 0.0030\n",
      "[305, 45]: MSE Loss: 0.0018\n",
      "[305, 60]: MSE Loss: 0.0025\n",
      "Epoch 306\n",
      "[306, 15]: MSE Loss: 0.0022\n",
      "[306, 30]: MSE Loss: 0.0022\n",
      "[306, 45]: MSE Loss: 0.0016\n",
      "[306, 60]: MSE Loss: 0.0019\n",
      "Epoch 307\n",
      "[307, 15]: MSE Loss: 0.0022\n",
      "[307, 30]: MSE Loss: 0.0016\n",
      "[307, 45]: MSE Loss: 0.0032\n",
      "[307, 60]: MSE Loss: 0.0015\n",
      "Epoch 308\n",
      "[308, 15]: MSE Loss: 0.0032\n",
      "[308, 30]: MSE Loss: 0.0019\n",
      "[308, 45]: MSE Loss: 0.0023\n",
      "[308, 60]: MSE Loss: 0.0028\n",
      "Epoch 309\n",
      "[309, 15]: MSE Loss: 0.0011\n",
      "[309, 30]: MSE Loss: 0.0023\n",
      "[309, 45]: MSE Loss: 0.0025\n",
      "[309, 60]: MSE Loss: 0.0016\n",
      "Epoch 310\n",
      "[310, 15]: MSE Loss: 0.0021\n",
      "[310, 30]: MSE Loss: 0.0039\n",
      "[310, 45]: MSE Loss: 0.0018\n",
      "[310, 60]: MSE Loss: 0.0017\n",
      "Test Loss: 0.05062\n",
      "########################################\n",
      "Epoch 311\n",
      "[311, 15]: MSE Loss: 0.0039\n",
      "[311, 30]: MSE Loss: 0.0019\n",
      "[311, 45]: MSE Loss: 0.0013\n",
      "[311, 60]: MSE Loss: 0.0019\n",
      "Epoch 312\n",
      "[312, 15]: MSE Loss: 0.0013\n",
      "[312, 30]: MSE Loss: 0.0035\n",
      "[312, 45]: MSE Loss: 0.0018\n",
      "[312, 60]: MSE Loss: 0.0022\n",
      "Epoch 313\n",
      "[313, 15]: MSE Loss: 0.0018\n",
      "[313, 30]: MSE Loss: 0.0022\n",
      "[313, 45]: MSE Loss: 0.0011\n",
      "[313, 60]: MSE Loss: 0.0020\n",
      "Epoch 314\n",
      "[314, 15]: MSE Loss: 0.0014\n",
      "[314, 30]: MSE Loss: 0.0027\n",
      "[314, 45]: MSE Loss: 0.0015\n",
      "[314, 60]: MSE Loss: 0.0014\n",
      "Epoch 315\n",
      "[315, 15]: MSE Loss: 0.0013\n",
      "[315, 30]: MSE Loss: 0.0012\n",
      "[315, 45]: MSE Loss: 0.0030\n",
      "[315, 60]: MSE Loss: 0.0045\n",
      "Epoch 316\n",
      "[316, 15]: MSE Loss: 0.0017\n",
      "[316, 30]: MSE Loss: 0.0047\n",
      "[316, 45]: MSE Loss: 0.0014\n",
      "[316, 60]: MSE Loss: 0.0015\n",
      "Epoch 317\n",
      "[317, 15]: MSE Loss: 0.0019\n",
      "[317, 30]: MSE Loss: 0.0011\n",
      "[317, 45]: MSE Loss: 0.0012\n",
      "[317, 60]: MSE Loss: 0.0017\n",
      "Epoch 318\n",
      "[318, 15]: MSE Loss: 0.0011\n",
      "[318, 30]: MSE Loss: 0.0021\n",
      "[318, 45]: MSE Loss: 0.0010\n",
      "[318, 60]: MSE Loss: 0.0018\n",
      "Epoch 319\n",
      "[319, 15]: MSE Loss: 0.0019\n",
      "[319, 30]: MSE Loss: 0.0018\n",
      "[319, 45]: MSE Loss: 0.0015\n",
      "[319, 60]: MSE Loss: 0.0012\n",
      "Epoch 320\n",
      "[320, 15]: MSE Loss: 0.0024\n",
      "[320, 30]: MSE Loss: 0.0014\n",
      "[320, 45]: MSE Loss: 0.0025\n",
      "[320, 60]: MSE Loss: 0.0022\n",
      "Test Loss: 0.05005\n",
      "########################################\n",
      "Epoch 321\n",
      "[321, 15]: MSE Loss: 0.0018\n",
      "[321, 30]: MSE Loss: 0.0030\n",
      "[321, 45]: MSE Loss: 0.0022\n",
      "[321, 60]: MSE Loss: 0.0017\n",
      "Epoch 322\n",
      "[322, 15]: MSE Loss: 0.0015\n",
      "[322, 30]: MSE Loss: 0.0014\n",
      "[322, 45]: MSE Loss: 0.0024\n",
      "[322, 60]: MSE Loss: 0.0015\n",
      "Epoch 323\n",
      "[323, 15]: MSE Loss: 0.0019\n",
      "[323, 30]: MSE Loss: 0.0014\n",
      "[323, 45]: MSE Loss: 0.0016\n",
      "[323, 60]: MSE Loss: 0.0017\n",
      "Epoch 324\n",
      "[324, 15]: MSE Loss: 0.0021\n",
      "[324, 30]: MSE Loss: 0.0020\n",
      "[324, 45]: MSE Loss: 0.0012\n",
      "[324, 60]: MSE Loss: 0.0012\n",
      "Epoch 325\n",
      "[325, 15]: MSE Loss: 0.0016\n",
      "[325, 30]: MSE Loss: 0.0022\n",
      "[325, 45]: MSE Loss: 0.0011\n",
      "[325, 60]: MSE Loss: 0.0027\n",
      "Epoch 326\n",
      "[326, 15]: MSE Loss: 0.0009\n",
      "[326, 30]: MSE Loss: 0.0012\n",
      "[326, 45]: MSE Loss: 0.0013\n",
      "[326, 60]: MSE Loss: 0.0010\n",
      "Epoch 327\n",
      "[327, 15]: MSE Loss: 0.0011\n",
      "[327, 30]: MSE Loss: 0.0017\n",
      "[327, 45]: MSE Loss: 0.0012\n",
      "[327, 60]: MSE Loss: 0.0011\n",
      "Epoch 328\n",
      "[328, 15]: MSE Loss: 0.0023\n",
      "[328, 30]: MSE Loss: 0.0015\n",
      "[328, 45]: MSE Loss: 0.0019\n",
      "[328, 60]: MSE Loss: 0.0012\n",
      "Epoch 329\n",
      "[329, 15]: MSE Loss: 0.0026\n",
      "[329, 30]: MSE Loss: 0.0036\n",
      "[329, 45]: MSE Loss: 0.0013\n",
      "[329, 60]: MSE Loss: 0.0021\n",
      "Epoch 330\n",
      "[330, 15]: MSE Loss: 0.0007\n",
      "[330, 30]: MSE Loss: 0.0020\n",
      "[330, 45]: MSE Loss: 0.0009\n",
      "[330, 60]: MSE Loss: 0.0078\n",
      "Test Loss: 0.04887\n",
      "########################################\n",
      "Epoch 331\n",
      "[331, 15]: MSE Loss: 0.0028\n",
      "[331, 30]: MSE Loss: 0.0017\n",
      "[331, 45]: MSE Loss: 0.0018\n",
      "[331, 60]: MSE Loss: 0.0012\n",
      "Epoch 332\n",
      "[332, 15]: MSE Loss: 0.0014\n",
      "[332, 30]: MSE Loss: 0.0045\n",
      "[332, 45]: MSE Loss: 0.0014\n",
      "[332, 60]: MSE Loss: 0.0019\n",
      "Epoch 333\n",
      "[333, 15]: MSE Loss: 0.0021\n",
      "[333, 30]: MSE Loss: 0.0013\n",
      "[333, 45]: MSE Loss: 0.0015\n",
      "[333, 60]: MSE Loss: 0.0033\n",
      "Epoch 334\n",
      "[334, 15]: MSE Loss: 0.0018\n",
      "[334, 30]: MSE Loss: 0.0019\n",
      "[334, 45]: MSE Loss: 0.0011\n",
      "[334, 60]: MSE Loss: 0.0018\n",
      "Epoch 335\n",
      "[335, 15]: MSE Loss: 0.0037\n",
      "[335, 30]: MSE Loss: 0.0014\n",
      "[335, 45]: MSE Loss: 0.0012\n",
      "[335, 60]: MSE Loss: 0.0016\n",
      "Epoch 336\n",
      "[336, 15]: MSE Loss: 0.0017\n",
      "[336, 30]: MSE Loss: 0.0021\n",
      "[336, 45]: MSE Loss: 0.0009\n",
      "[336, 60]: MSE Loss: 0.0017\n",
      "Epoch 337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[337, 15]: MSE Loss: 0.0015\n",
      "[337, 30]: MSE Loss: 0.0040\n",
      "[337, 45]: MSE Loss: 0.0019\n",
      "[337, 60]: MSE Loss: 0.0013\n",
      "Epoch 338\n",
      "[338, 15]: MSE Loss: 0.0018\n",
      "[338, 30]: MSE Loss: 0.0010\n",
      "[338, 45]: MSE Loss: 0.0014\n",
      "[338, 60]: MSE Loss: 0.0019\n",
      "Epoch 339\n",
      "[339, 15]: MSE Loss: 0.0018\n",
      "[339, 30]: MSE Loss: 0.0016\n",
      "[339, 45]: MSE Loss: 0.0025\n",
      "[339, 60]: MSE Loss: 0.0014\n",
      "Epoch 340\n",
      "[340, 15]: MSE Loss: 0.0013\n",
      "[340, 30]: MSE Loss: 0.0025\n",
      "[340, 45]: MSE Loss: 0.0015\n",
      "[340, 60]: MSE Loss: 0.0024\n",
      "Test Loss: 0.04969\n",
      "########################################\n",
      "Epoch 341\n",
      "[341, 15]: MSE Loss: 0.0026\n",
      "[341, 30]: MSE Loss: 0.0012\n",
      "[341, 45]: MSE Loss: 0.0013\n",
      "[341, 60]: MSE Loss: 0.0011\n",
      "Epoch 342\n",
      "[342, 15]: MSE Loss: 0.0015\n",
      "[342, 30]: MSE Loss: 0.0022\n",
      "[342, 45]: MSE Loss: 0.0028\n",
      "[342, 60]: MSE Loss: 0.0010\n",
      "Epoch 343\n",
      "[343, 15]: MSE Loss: 0.0011\n",
      "[343, 30]: MSE Loss: 0.0017\n",
      "[343, 45]: MSE Loss: 0.0010\n",
      "[343, 60]: MSE Loss: 0.0018\n",
      "Epoch 344\n",
      "[344, 15]: MSE Loss: 0.0014\n",
      "[344, 30]: MSE Loss: 0.0030\n",
      "[344, 45]: MSE Loss: 0.0025\n",
      "[344, 60]: MSE Loss: 0.0032\n",
      "Epoch 345\n",
      "[345, 15]: MSE Loss: 0.0013\n",
      "[345, 30]: MSE Loss: 0.0031\n",
      "[345, 45]: MSE Loss: 0.0014\n",
      "[345, 60]: MSE Loss: 0.0049\n",
      "Epoch 346\n",
      "[346, 15]: MSE Loss: 0.0022\n",
      "[346, 30]: MSE Loss: 0.0027\n",
      "[346, 45]: MSE Loss: 0.0015\n",
      "[346, 60]: MSE Loss: 0.0012\n",
      "Epoch 347\n",
      "[347, 15]: MSE Loss: 0.0014\n",
      "[347, 30]: MSE Loss: 0.0012\n",
      "[347, 45]: MSE Loss: 0.0015\n",
      "[347, 60]: MSE Loss: 0.0022\n",
      "Epoch 348\n",
      "[348, 15]: MSE Loss: 0.0028\n",
      "[348, 30]: MSE Loss: 0.0019\n",
      "[348, 45]: MSE Loss: 0.0021\n",
      "[348, 60]: MSE Loss: 0.0017\n",
      "Epoch 349\n",
      "[349, 15]: MSE Loss: 0.0028\n",
      "[349, 30]: MSE Loss: 0.0021\n",
      "[349, 45]: MSE Loss: 0.0011\n",
      "[349, 60]: MSE Loss: 0.0028\n",
      "Epoch 350\n",
      "[350, 15]: MSE Loss: 0.0022\n",
      "[350, 30]: MSE Loss: 0.0019\n",
      "[350, 45]: MSE Loss: 0.0016\n",
      "[350, 60]: MSE Loss: 0.0023\n",
      "Test Loss: 0.04847\n",
      "########################################\n",
      "Epoch 351\n",
      "[351, 15]: MSE Loss: 0.0022\n",
      "[351, 30]: MSE Loss: 0.0014\n",
      "[351, 45]: MSE Loss: 0.0028\n",
      "[351, 60]: MSE Loss: 0.0014\n",
      "Epoch 352\n",
      "[352, 15]: MSE Loss: 0.0017\n",
      "[352, 30]: MSE Loss: 0.0013\n",
      "[352, 45]: MSE Loss: 0.0013\n",
      "[352, 60]: MSE Loss: 0.0022\n",
      "Epoch 353\n",
      "[353, 15]: MSE Loss: 0.0012\n",
      "[353, 30]: MSE Loss: 0.0009\n",
      "[353, 45]: MSE Loss: 0.0016\n",
      "[353, 60]: MSE Loss: 0.0018\n",
      "Epoch 354\n",
      "[354, 15]: MSE Loss: 0.0023\n",
      "[354, 30]: MSE Loss: 0.0012\n",
      "[354, 45]: MSE Loss: 0.0022\n",
      "[354, 60]: MSE Loss: 0.0023\n",
      "Epoch 355\n",
      "[355, 15]: MSE Loss: 0.0018\n",
      "[355, 30]: MSE Loss: 0.0011\n",
      "[355, 45]: MSE Loss: 0.0020\n",
      "[355, 60]: MSE Loss: 0.0011\n",
      "Epoch 356\n",
      "[356, 15]: MSE Loss: 0.0011\n",
      "[356, 30]: MSE Loss: 0.0020\n",
      "[356, 45]: MSE Loss: 0.0017\n",
      "[356, 60]: MSE Loss: 0.0224\n",
      "Epoch 357\n",
      "[357, 15]: MSE Loss: 0.0038\n",
      "[357, 30]: MSE Loss: 0.0015\n",
      "[357, 45]: MSE Loss: 0.0022\n",
      "[357, 60]: MSE Loss: 0.0018\n",
      "Epoch 358\n",
      "[358, 15]: MSE Loss: 0.0020\n",
      "[358, 30]: MSE Loss: 0.0015\n",
      "[358, 45]: MSE Loss: 0.0011\n",
      "[358, 60]: MSE Loss: 0.0027\n",
      "Epoch 359\n",
      "[359, 15]: MSE Loss: 0.0024\n",
      "[359, 30]: MSE Loss: 0.0021\n",
      "[359, 45]: MSE Loss: 0.0015\n",
      "[359, 60]: MSE Loss: 0.0017\n",
      "Epoch 360\n",
      "[360, 15]: MSE Loss: 0.0088\n",
      "[360, 30]: MSE Loss: 0.0022\n",
      "[360, 45]: MSE Loss: 0.0017\n",
      "[360, 60]: MSE Loss: 0.0021\n",
      "Test Loss: 0.05256\n",
      "########################################\n",
      "Epoch 361\n",
      "[361, 15]: MSE Loss: 0.0018\n",
      "[361, 30]: MSE Loss: 0.0010\n",
      "[361, 45]: MSE Loss: 0.0022\n",
      "[361, 60]: MSE Loss: 0.0026\n",
      "Epoch 362\n",
      "[362, 15]: MSE Loss: 0.0061\n",
      "[362, 30]: MSE Loss: 0.0019\n",
      "[362, 45]: MSE Loss: 0.0016\n",
      "[362, 60]: MSE Loss: 0.0026\n",
      "Epoch 363\n",
      "[363, 15]: MSE Loss: 0.0036\n",
      "[363, 30]: MSE Loss: 0.0032\n",
      "[363, 45]: MSE Loss: 0.0062\n",
      "[363, 60]: MSE Loss: 0.0055\n",
      "Epoch 364\n",
      "[364, 15]: MSE Loss: 0.0039\n",
      "[364, 30]: MSE Loss: 0.0033\n",
      "[364, 45]: MSE Loss: 0.0036\n",
      "[364, 60]: MSE Loss: 0.0033\n",
      "Epoch 365\n",
      "[365, 15]: MSE Loss: 0.0024\n",
      "[365, 30]: MSE Loss: 0.0033\n",
      "[365, 45]: MSE Loss: 0.0028\n",
      "[365, 60]: MSE Loss: 0.0021\n",
      "Epoch 366\n",
      "[366, 15]: MSE Loss: 0.0022\n",
      "[366, 30]: MSE Loss: 0.0025\n",
      "[366, 45]: MSE Loss: 0.0035\n",
      "[366, 60]: MSE Loss: 0.0021\n",
      "Epoch 367\n",
      "[367, 15]: MSE Loss: 0.0029\n",
      "[367, 30]: MSE Loss: 0.0030\n",
      "[367, 45]: MSE Loss: 0.0015\n",
      "[367, 60]: MSE Loss: 0.0016\n",
      "Epoch 368\n",
      "[368, 15]: MSE Loss: 0.0028\n",
      "[368, 30]: MSE Loss: 0.0030\n",
      "[368, 45]: MSE Loss: 0.0015\n",
      "[368, 60]: MSE Loss: 0.0016\n",
      "Epoch 369\n",
      "[369, 15]: MSE Loss: 0.0043\n",
      "[369, 30]: MSE Loss: 0.0024\n",
      "[369, 45]: MSE Loss: 0.0014\n",
      "[369, 60]: MSE Loss: 0.0018\n",
      "Epoch 370\n",
      "[370, 15]: MSE Loss: 0.0009\n",
      "[370, 30]: MSE Loss: 0.0012\n",
      "[370, 45]: MSE Loss: 0.0033\n",
      "[370, 60]: MSE Loss: 0.0019\n",
      "Test Loss: 0.05651\n",
      "########################################\n",
      "Epoch 371\n",
      "[371, 15]: MSE Loss: 0.0009\n",
      "[371, 30]: MSE Loss: 0.0021\n",
      "[371, 45]: MSE Loss: 0.0030\n",
      "[371, 60]: MSE Loss: 0.0015\n",
      "Epoch 372\n",
      "[372, 15]: MSE Loss: 0.0017\n",
      "[372, 30]: MSE Loss: 0.0021\n",
      "[372, 45]: MSE Loss: 0.0016\n",
      "[372, 60]: MSE Loss: 0.0037\n",
      "Epoch 373\n",
      "[373, 15]: MSE Loss: 0.0036\n",
      "[373, 30]: MSE Loss: 0.0016\n",
      "[373, 45]: MSE Loss: 0.0029\n",
      "[373, 60]: MSE Loss: 0.0017\n",
      "Epoch 374\n",
      "[374, 15]: MSE Loss: 0.0010\n",
      "[374, 30]: MSE Loss: 0.0009\n",
      "[374, 45]: MSE Loss: 0.0016\n",
      "[374, 60]: MSE Loss: 0.0013\n",
      "Epoch 375\n",
      "[375, 15]: MSE Loss: 0.0020\n",
      "[375, 30]: MSE Loss: 0.0022\n",
      "[375, 45]: MSE Loss: 0.0009\n",
      "[375, 60]: MSE Loss: 0.0010\n",
      "Epoch 376\n",
      "[376, 15]: MSE Loss: 0.0023\n",
      "[376, 30]: MSE Loss: 0.0012\n",
      "[376, 45]: MSE Loss: 0.0018\n",
      "[376, 60]: MSE Loss: 0.0012\n",
      "Epoch 377\n",
      "[377, 15]: MSE Loss: 0.0016\n",
      "[377, 30]: MSE Loss: 0.0015\n",
      "[377, 45]: MSE Loss: 0.0022\n",
      "[377, 60]: MSE Loss: 0.0018\n",
      "Epoch 378\n",
      "[378, 15]: MSE Loss: 0.0012\n",
      "[378, 30]: MSE Loss: 0.0014\n",
      "[378, 45]: MSE Loss: 0.0011\n",
      "[378, 60]: MSE Loss: 0.0013\n",
      "Epoch 379\n",
      "[379, 15]: MSE Loss: 0.0013\n",
      "[379, 30]: MSE Loss: 0.0020\n",
      "[379, 45]: MSE Loss: 0.0026\n",
      "[379, 60]: MSE Loss: 0.0017\n",
      "Epoch 380\n",
      "[380, 15]: MSE Loss: 0.0030\n",
      "[380, 30]: MSE Loss: 0.0032\n",
      "[380, 45]: MSE Loss: 0.0018\n",
      "[380, 60]: MSE Loss: 0.0024\n",
      "Test Loss: 0.05025\n",
      "########################################\n",
      "Epoch 381\n",
      "[381, 15]: MSE Loss: 0.0017\n",
      "[381, 30]: MSE Loss: 0.0014\n",
      "[381, 45]: MSE Loss: 0.0010\n",
      "[381, 60]: MSE Loss: 0.0018\n",
      "Epoch 382\n",
      "[382, 15]: MSE Loss: 0.0013\n",
      "[382, 30]: MSE Loss: 0.0048\n",
      "[382, 45]: MSE Loss: 0.0016\n",
      "[382, 60]: MSE Loss: 0.0010\n",
      "Epoch 383\n",
      "[383, 15]: MSE Loss: 0.0018\n",
      "[383, 30]: MSE Loss: 0.0016\n",
      "[383, 45]: MSE Loss: 0.0016\n",
      "[383, 60]: MSE Loss: 0.0017\n",
      "Epoch 384\n",
      "[384, 15]: MSE Loss: 0.0035\n",
      "[384, 30]: MSE Loss: 0.0014\n",
      "[384, 45]: MSE Loss: 0.0022\n",
      "[384, 60]: MSE Loss: 0.0011\n",
      "Epoch 385\n",
      "[385, 15]: MSE Loss: 0.0009\n",
      "[385, 30]: MSE Loss: 0.0010\n",
      "[385, 45]: MSE Loss: 0.0017\n",
      "[385, 60]: MSE Loss: 0.0019\n",
      "Epoch 386\n",
      "[386, 15]: MSE Loss: 0.0010\n",
      "[386, 30]: MSE Loss: 0.0016\n",
      "[386, 45]: MSE Loss: 0.0024\n",
      "[386, 60]: MSE Loss: 0.0021\n",
      "Epoch 387\n",
      "[387, 15]: MSE Loss: 0.0023\n",
      "[387, 30]: MSE Loss: 0.0013\n",
      "[387, 45]: MSE Loss: 0.0020\n",
      "[387, 60]: MSE Loss: 0.0027\n",
      "Epoch 388\n",
      "[388, 15]: MSE Loss: 0.0021\n",
      "[388, 30]: MSE Loss: 0.0018\n",
      "[388, 45]: MSE Loss: 0.0014\n",
      "[388, 60]: MSE Loss: 0.0017\n",
      "Epoch 389\n",
      "[389, 15]: MSE Loss: 0.0023\n",
      "[389, 30]: MSE Loss: 0.0022\n",
      "[389, 45]: MSE Loss: 0.0012\n",
      "[389, 60]: MSE Loss: 0.0033\n",
      "Epoch 390\n",
      "[390, 15]: MSE Loss: 0.0016\n",
      "[390, 30]: MSE Loss: 0.0018\n",
      "[390, 45]: MSE Loss: 0.0031\n",
      "[390, 60]: MSE Loss: 0.0030\n",
      "Test Loss: 0.04827\n",
      "########################################\n",
      "Epoch 391\n",
      "[391, 15]: MSE Loss: 0.0013\n",
      "[391, 30]: MSE Loss: 0.0011\n",
      "[391, 45]: MSE Loss: 0.0011\n",
      "[391, 60]: MSE Loss: 0.0020\n",
      "Epoch 392\n",
      "[392, 15]: MSE Loss: 0.0029\n",
      "[392, 30]: MSE Loss: 0.0020\n",
      "[392, 45]: MSE Loss: 0.0020\n",
      "[392, 60]: MSE Loss: 0.0012\n",
      "Epoch 393\n",
      "[393, 15]: MSE Loss: 0.0022\n",
      "[393, 30]: MSE Loss: 0.0014\n",
      "[393, 45]: MSE Loss: 0.0025\n",
      "[393, 60]: MSE Loss: 0.0013\n",
      "Epoch 394\n",
      "[394, 15]: MSE Loss: 0.0017\n",
      "[394, 30]: MSE Loss: 0.0016\n",
      "[394, 45]: MSE Loss: 0.0021\n",
      "[394, 60]: MSE Loss: 0.0016\n",
      "Epoch 395\n",
      "[395, 15]: MSE Loss: 0.0045\n",
      "[395, 30]: MSE Loss: 0.0014\n",
      "[395, 45]: MSE Loss: 0.0012\n",
      "[395, 60]: MSE Loss: 0.0014\n",
      "Epoch 396\n",
      "[396, 15]: MSE Loss: 0.0013\n",
      "[396, 30]: MSE Loss: 0.0017\n",
      "[396, 45]: MSE Loss: 0.0019\n",
      "[396, 60]: MSE Loss: 0.0016\n",
      "Epoch 397\n",
      "[397, 15]: MSE Loss: 0.0016\n",
      "[397, 30]: MSE Loss: 0.0034\n",
      "[397, 45]: MSE Loss: 0.0015\n",
      "[397, 60]: MSE Loss: 0.0019\n",
      "Epoch 398\n",
      "[398, 15]: MSE Loss: 0.0046\n",
      "[398, 30]: MSE Loss: 0.0025\n",
      "[398, 45]: MSE Loss: 0.0023\n",
      "[398, 60]: MSE Loss: 0.0013\n",
      "Epoch 399\n",
      "[399, 15]: MSE Loss: 0.0032\n",
      "[399, 30]: MSE Loss: 0.0020\n",
      "[399, 45]: MSE Loss: 0.0012\n",
      "[399, 60]: MSE Loss: 0.0021\n",
      "Epoch 400\n",
      "[400, 15]: MSE Loss: 0.0015\n",
      "[400, 30]: MSE Loss: 0.0031\n",
      "[400, 45]: MSE Loss: 0.0013\n",
      "[400, 60]: MSE Loss: 0.0023\n",
      "Test Loss: 0.05399\n",
      "########################################\n",
      "Epoch 401\n",
      "[401, 15]: MSE Loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[401, 30]: MSE Loss: 0.0011\n",
      "[401, 45]: MSE Loss: 0.0020\n",
      "[401, 60]: MSE Loss: 0.0016\n",
      "Epoch 402\n",
      "[402, 15]: MSE Loss: 0.0022\n",
      "[402, 30]: MSE Loss: 0.0012\n",
      "[402, 45]: MSE Loss: 0.0021\n",
      "[402, 60]: MSE Loss: 0.0011\n",
      "Epoch 403\n",
      "[403, 15]: MSE Loss: 0.0010\n",
      "[403, 30]: MSE Loss: 0.0020\n",
      "[403, 45]: MSE Loss: 0.0023\n",
      "[403, 60]: MSE Loss: 0.0013\n",
      "Epoch 404\n",
      "[404, 15]: MSE Loss: 0.0019\n",
      "[404, 30]: MSE Loss: 0.0017\n",
      "[404, 45]: MSE Loss: 0.0076\n",
      "[404, 60]: MSE Loss: 0.0011\n",
      "Epoch 405\n",
      "[405, 15]: MSE Loss: 0.0020\n",
      "[405, 30]: MSE Loss: 0.0013\n",
      "[405, 45]: MSE Loss: 0.0007\n",
      "[405, 60]: MSE Loss: 0.0021\n",
      "Epoch 406\n",
      "[406, 15]: MSE Loss: 0.0019\n",
      "[406, 30]: MSE Loss: 0.0017\n",
      "[406, 45]: MSE Loss: 0.0053\n",
      "[406, 60]: MSE Loss: 0.0030\n",
      "Epoch 407\n",
      "[407, 15]: MSE Loss: 0.0022\n",
      "[407, 30]: MSE Loss: 0.0013\n",
      "[407, 45]: MSE Loss: 0.0011\n",
      "[407, 60]: MSE Loss: 0.0023\n",
      "Epoch 408\n",
      "[408, 15]: MSE Loss: 0.0006\n",
      "[408, 30]: MSE Loss: 0.0019\n",
      "[408, 45]: MSE Loss: 0.0018\n",
      "[408, 60]: MSE Loss: 0.0014\n",
      "Epoch 409\n",
      "[409, 15]: MSE Loss: 0.0015\n",
      "[409, 30]: MSE Loss: 0.0012\n",
      "[409, 45]: MSE Loss: 0.0014\n",
      "[409, 60]: MSE Loss: 0.0022\n",
      "Epoch 410\n",
      "[410, 15]: MSE Loss: 0.0012\n",
      "[410, 30]: MSE Loss: 0.0017\n",
      "[410, 45]: MSE Loss: 0.0015\n",
      "[410, 60]: MSE Loss: 0.0034\n",
      "Test Loss: 0.05447\n",
      "########################################\n",
      "Epoch 411\n",
      "[411, 15]: MSE Loss: 0.0014\n",
      "[411, 30]: MSE Loss: 0.0028\n",
      "[411, 45]: MSE Loss: 0.0012\n",
      "[411, 60]: MSE Loss: 0.0015\n",
      "Epoch 412\n",
      "[412, 15]: MSE Loss: 0.0015\n",
      "[412, 30]: MSE Loss: 0.0020\n",
      "[412, 45]: MSE Loss: 0.0009\n",
      "[412, 60]: MSE Loss: 0.0009\n",
      "Epoch 413\n",
      "[413, 15]: MSE Loss: 0.0017\n",
      "[413, 30]: MSE Loss: 0.0019\n",
      "[413, 45]: MSE Loss: 0.0023\n",
      "[413, 60]: MSE Loss: 0.0024\n",
      "Epoch 414\n",
      "[414, 15]: MSE Loss: 0.0017\n",
      "[414, 30]: MSE Loss: 0.0011\n",
      "[414, 45]: MSE Loss: 0.0026\n",
      "[414, 60]: MSE Loss: 0.0014\n",
      "Epoch 415\n",
      "[415, 15]: MSE Loss: 0.0019\n",
      "[415, 30]: MSE Loss: 0.0010\n",
      "[415, 45]: MSE Loss: 0.0011\n",
      "[415, 60]: MSE Loss: 0.0022\n",
      "Epoch 416\n",
      "[416, 15]: MSE Loss: 0.0014\n",
      "[416, 30]: MSE Loss: 0.0018\n",
      "[416, 45]: MSE Loss: 0.0014\n",
      "[416, 60]: MSE Loss: 0.0014\n",
      "Epoch 417\n",
      "[417, 15]: MSE Loss: 0.0013\n",
      "[417, 30]: MSE Loss: 0.0014\n",
      "[417, 45]: MSE Loss: 0.0012\n",
      "[417, 60]: MSE Loss: 0.0019\n",
      "Epoch 418\n",
      "[418, 15]: MSE Loss: 0.0031\n",
      "[418, 30]: MSE Loss: 0.0017\n",
      "[418, 45]: MSE Loss: 0.0018\n",
      "[418, 60]: MSE Loss: 0.0020\n",
      "Epoch 419\n",
      "[419, 15]: MSE Loss: 0.0051\n",
      "[419, 30]: MSE Loss: 0.0012\n",
      "[419, 45]: MSE Loss: 0.0010\n",
      "[419, 60]: MSE Loss: 0.0011\n",
      "Epoch 420\n",
      "[420, 15]: MSE Loss: 0.0020\n",
      "[420, 30]: MSE Loss: 0.0027\n",
      "[420, 45]: MSE Loss: 0.0030\n",
      "[420, 60]: MSE Loss: 0.0014\n",
      "Test Loss: 0.05079\n",
      "########################################\n",
      "Epoch 421\n",
      "[421, 15]: MSE Loss: 0.0012\n",
      "[421, 30]: MSE Loss: 0.0021\n",
      "[421, 45]: MSE Loss: 0.0011\n",
      "[421, 60]: MSE Loss: 0.0012\n",
      "Epoch 422\n",
      "[422, 15]: MSE Loss: 0.0019\n",
      "[422, 30]: MSE Loss: 0.0013\n",
      "[422, 45]: MSE Loss: 0.0018\n",
      "[422, 60]: MSE Loss: 0.0016\n",
      "Epoch 423\n",
      "[423, 15]: MSE Loss: 0.0022\n",
      "[423, 30]: MSE Loss: 0.0011\n",
      "[423, 45]: MSE Loss: 0.0013\n",
      "[423, 60]: MSE Loss: 0.0011\n",
      "Epoch 424\n",
      "[424, 15]: MSE Loss: 0.0016\n",
      "[424, 30]: MSE Loss: 0.0015\n",
      "[424, 45]: MSE Loss: 0.0023\n",
      "[424, 60]: MSE Loss: 0.0024\n",
      "Epoch 425\n",
      "[425, 15]: MSE Loss: 0.0009\n",
      "[425, 30]: MSE Loss: 0.0016\n",
      "[425, 45]: MSE Loss: 0.0013\n",
      "[425, 60]: MSE Loss: 0.0019\n",
      "Epoch 426\n",
      "[426, 15]: MSE Loss: 0.0021\n",
      "[426, 30]: MSE Loss: 0.0020\n",
      "[426, 45]: MSE Loss: 0.0010\n",
      "[426, 60]: MSE Loss: 0.0010\n",
      "Epoch 427\n",
      "[427, 15]: MSE Loss: 0.0013\n",
      "[427, 30]: MSE Loss: 0.0020\n",
      "[427, 45]: MSE Loss: 0.0011\n",
      "[427, 60]: MSE Loss: 0.0023\n",
      "Epoch 428\n",
      "[428, 15]: MSE Loss: 0.0012\n",
      "[428, 30]: MSE Loss: 0.0026\n",
      "[428, 45]: MSE Loss: 0.0020\n",
      "[428, 60]: MSE Loss: 0.0019\n",
      "Epoch 429\n",
      "[429, 15]: MSE Loss: 0.0016\n",
      "[429, 30]: MSE Loss: 0.0034\n",
      "[429, 45]: MSE Loss: 0.0022\n",
      "[429, 60]: MSE Loss: 0.0020\n",
      "Epoch 430\n",
      "[430, 15]: MSE Loss: 0.0016\n",
      "[430, 30]: MSE Loss: 0.0013\n",
      "[430, 45]: MSE Loss: 0.0013\n",
      "[430, 60]: MSE Loss: 0.0018\n",
      "Test Loss: 0.04802\n",
      "########################################\n",
      "Epoch 431\n",
      "[431, 15]: MSE Loss: 0.0023\n",
      "[431, 30]: MSE Loss: 0.0022\n",
      "[431, 45]: MSE Loss: 0.0025\n",
      "[431, 60]: MSE Loss: 0.0020\n",
      "Epoch 432\n",
      "[432, 15]: MSE Loss: 0.0025\n",
      "[432, 30]: MSE Loss: 0.0017\n",
      "[432, 45]: MSE Loss: 0.0021\n",
      "[432, 60]: MSE Loss: 0.0027\n",
      "Epoch 433\n",
      "[433, 15]: MSE Loss: 0.0034\n",
      "[433, 30]: MSE Loss: 0.0024\n",
      "[433, 45]: MSE Loss: 0.0018\n",
      "[433, 60]: MSE Loss: 0.0015\n",
      "Epoch 434\n",
      "[434, 15]: MSE Loss: 0.0027\n",
      "[434, 30]: MSE Loss: 0.0020\n",
      "[434, 45]: MSE Loss: 0.0015\n",
      "[434, 60]: MSE Loss: 0.0027\n",
      "Epoch 435\n",
      "[435, 15]: MSE Loss: 0.0021\n",
      "[435, 30]: MSE Loss: 0.0017\n",
      "[435, 45]: MSE Loss: 0.0010\n",
      "[435, 60]: MSE Loss: 0.0022\n",
      "Epoch 436\n",
      "[436, 15]: MSE Loss: 0.0021\n",
      "[436, 30]: MSE Loss: 0.0010\n",
      "[436, 45]: MSE Loss: 0.0043\n",
      "[436, 60]: MSE Loss: 0.0031\n",
      "Epoch 437\n",
      "[437, 15]: MSE Loss: 0.0024\n",
      "[437, 30]: MSE Loss: 0.0013\n",
      "[437, 45]: MSE Loss: 0.0017\n",
      "[437, 60]: MSE Loss: 0.0012\n",
      "Epoch 438\n",
      "[438, 15]: MSE Loss: 0.0009\n",
      "[438, 30]: MSE Loss: 0.0022\n",
      "[438, 45]: MSE Loss: 0.0007\n",
      "[438, 60]: MSE Loss: 0.0012\n",
      "Epoch 439\n",
      "[439, 15]: MSE Loss: 0.0018\n",
      "[439, 30]: MSE Loss: 0.0019\n",
      "[439, 45]: MSE Loss: 0.0011\n",
      "[439, 60]: MSE Loss: 0.0013\n",
      "Epoch 440\n",
      "[440, 15]: MSE Loss: 0.0010\n",
      "[440, 30]: MSE Loss: 0.0013\n",
      "[440, 45]: MSE Loss: 0.0027\n",
      "[440, 60]: MSE Loss: 0.0015\n",
      "Test Loss: 0.04967\n",
      "########################################\n",
      "Epoch 441\n",
      "[441, 15]: MSE Loss: 0.0015\n",
      "[441, 30]: MSE Loss: 0.0015\n",
      "[441, 45]: MSE Loss: 0.0010\n",
      "[441, 60]: MSE Loss: 0.0010\n",
      "Epoch 442\n",
      "[442, 15]: MSE Loss: 0.0015\n",
      "[442, 30]: MSE Loss: 0.0013\n",
      "[442, 45]: MSE Loss: 0.0014\n",
      "[442, 60]: MSE Loss: 0.0011\n",
      "Epoch 443\n",
      "[443, 15]: MSE Loss: 0.0009\n",
      "[443, 30]: MSE Loss: 0.0020\n",
      "[443, 45]: MSE Loss: 0.0032\n",
      "[443, 60]: MSE Loss: 0.0018\n",
      "Epoch 444\n",
      "[444, 15]: MSE Loss: 0.0017\n",
      "[444, 30]: MSE Loss: 0.0021\n",
      "[444, 45]: MSE Loss: 0.0052\n",
      "[444, 60]: MSE Loss: 0.0020\n",
      "Epoch 445\n",
      "[445, 15]: MSE Loss: 0.0079\n",
      "[445, 30]: MSE Loss: 0.0023\n",
      "[445, 45]: MSE Loss: 0.0025\n",
      "[445, 60]: MSE Loss: 0.0021\n",
      "Epoch 446\n",
      "[446, 15]: MSE Loss: 0.0016\n",
      "[446, 30]: MSE Loss: 0.0012\n",
      "[446, 45]: MSE Loss: 0.0017\n",
      "[446, 60]: MSE Loss: 0.0036\n",
      "Epoch 447\n",
      "[447, 15]: MSE Loss: 0.0025\n",
      "[447, 30]: MSE Loss: 0.0061\n",
      "[447, 45]: MSE Loss: 0.0037\n",
      "[447, 60]: MSE Loss: 0.0035\n",
      "Epoch 448\n",
      "[448, 15]: MSE Loss: 0.0026\n",
      "[448, 30]: MSE Loss: 0.0045\n",
      "[448, 45]: MSE Loss: 0.0027\n",
      "[448, 60]: MSE Loss: 0.0040\n",
      "Epoch 449\n",
      "[449, 15]: MSE Loss: 0.0022\n",
      "[449, 30]: MSE Loss: 0.0027\n",
      "[449, 45]: MSE Loss: 0.0019\n",
      "[449, 60]: MSE Loss: 0.0024\n",
      "Epoch 450\n",
      "[450, 15]: MSE Loss: 0.0027\n",
      "[450, 30]: MSE Loss: 0.0013\n",
      "[450, 45]: MSE Loss: 0.0012\n",
      "[450, 60]: MSE Loss: 0.0051\n",
      "Test Loss: 0.12169\n",
      "########################################\n",
      "Epoch 451\n",
      "[451, 15]: MSE Loss: 0.0051\n",
      "[451, 30]: MSE Loss: 0.0027\n",
      "[451, 45]: MSE Loss: 0.0027\n",
      "[451, 60]: MSE Loss: 0.0017\n",
      "Epoch 452\n",
      "[452, 15]: MSE Loss: 0.0015\n",
      "[452, 30]: MSE Loss: 0.0010\n",
      "[452, 45]: MSE Loss: 0.0017\n",
      "[452, 60]: MSE Loss: 0.0012\n",
      "Epoch 453\n",
      "[453, 15]: MSE Loss: 0.0036\n",
      "[453, 30]: MSE Loss: 0.0026\n",
      "[453, 45]: MSE Loss: 0.0027\n",
      "[453, 60]: MSE Loss: 0.0030\n",
      "Epoch 454\n",
      "[454, 15]: MSE Loss: 0.0012\n",
      "[454, 30]: MSE Loss: 0.0028\n",
      "[454, 45]: MSE Loss: 0.0015\n",
      "[454, 60]: MSE Loss: 0.0017\n",
      "Epoch 455\n",
      "[455, 15]: MSE Loss: 0.0017\n",
      "[455, 30]: MSE Loss: 0.0024\n",
      "[455, 45]: MSE Loss: 0.0014\n",
      "[455, 60]: MSE Loss: 0.0012\n",
      "Epoch 456\n",
      "[456, 15]: MSE Loss: 0.0012\n",
      "[456, 30]: MSE Loss: 0.0014\n",
      "[456, 45]: MSE Loss: 0.0009\n",
      "[456, 60]: MSE Loss: 0.0014\n",
      "Epoch 457\n",
      "[457, 15]: MSE Loss: 0.0016\n",
      "[457, 30]: MSE Loss: 0.0021\n",
      "[457, 45]: MSE Loss: 0.0013\n",
      "[457, 60]: MSE Loss: 0.0014\n",
      "Epoch 458\n",
      "[458, 15]: MSE Loss: 0.0011\n",
      "[458, 30]: MSE Loss: 0.0014\n",
      "[458, 45]: MSE Loss: 0.0015\n",
      "[458, 60]: MSE Loss: 0.0011\n",
      "Epoch 459\n",
      "[459, 15]: MSE Loss: 0.0011\n",
      "[459, 30]: MSE Loss: 0.0010\n",
      "[459, 45]: MSE Loss: 0.0010\n",
      "[459, 60]: MSE Loss: 0.0030\n",
      "Epoch 460\n",
      "[460, 15]: MSE Loss: 0.0018\n",
      "[460, 30]: MSE Loss: 0.0016\n",
      "[460, 45]: MSE Loss: 0.0014\n",
      "[460, 60]: MSE Loss: 0.0011\n",
      "Test Loss: 0.05192\n",
      "########################################\n",
      "Epoch 461\n",
      "[461, 15]: MSE Loss: 0.0014\n",
      "[461, 30]: MSE Loss: 0.0015\n",
      "[461, 45]: MSE Loss: 0.0018\n",
      "[461, 60]: MSE Loss: 0.0018\n",
      "Epoch 462\n",
      "[462, 15]: MSE Loss: 0.0017\n",
      "[462, 30]: MSE Loss: 0.0022\n",
      "[462, 45]: MSE Loss: 0.0025\n",
      "[462, 60]: MSE Loss: 0.0014\n",
      "Epoch 463\n",
      "[463, 15]: MSE Loss: 0.0014\n",
      "[463, 30]: MSE Loss: 0.0017\n",
      "[463, 45]: MSE Loss: 0.0009\n",
      "[463, 60]: MSE Loss: 0.0019\n",
      "Epoch 464\n",
      "[464, 15]: MSE Loss: 0.0010\n",
      "[464, 30]: MSE Loss: 0.0010\n",
      "[464, 45]: MSE Loss: 0.0015\n",
      "[464, 60]: MSE Loss: 0.0014\n",
      "Epoch 465\n",
      "[465, 15]: MSE Loss: 0.0014\n",
      "[465, 30]: MSE Loss: 0.0010\n",
      "[465, 45]: MSE Loss: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[465, 60]: MSE Loss: 0.0019\n",
      "Epoch 466\n",
      "[466, 15]: MSE Loss: 0.0011\n",
      "[466, 30]: MSE Loss: 0.0025\n",
      "[466, 45]: MSE Loss: 0.0013\n",
      "[466, 60]: MSE Loss: 0.0021\n",
      "Epoch 467\n",
      "[467, 15]: MSE Loss: 0.0023\n",
      "[467, 30]: MSE Loss: 0.0016\n",
      "[467, 45]: MSE Loss: 0.0015\n",
      "[467, 60]: MSE Loss: 0.0025\n",
      "Epoch 468\n",
      "[468, 15]: MSE Loss: 0.0028\n",
      "[468, 30]: MSE Loss: 0.0021\n",
      "[468, 45]: MSE Loss: 0.0020\n",
      "[468, 60]: MSE Loss: 0.0019\n",
      "Epoch 469\n",
      "[469, 15]: MSE Loss: 0.0018\n",
      "[469, 30]: MSE Loss: 0.0009\n",
      "[469, 45]: MSE Loss: 0.0016\n",
      "[469, 60]: MSE Loss: 0.0017\n",
      "Epoch 470\n",
      "[470, 15]: MSE Loss: 0.0010\n",
      "[470, 30]: MSE Loss: 0.0024\n",
      "[470, 45]: MSE Loss: 0.0025\n",
      "[470, 60]: MSE Loss: 0.0017\n",
      "Test Loss: 0.04838\n",
      "########################################\n",
      "Epoch 471\n",
      "[471, 15]: MSE Loss: 0.0025\n",
      "[471, 30]: MSE Loss: 0.0022\n",
      "[471, 45]: MSE Loss: 0.0023\n",
      "[471, 60]: MSE Loss: 0.0013\n",
      "Epoch 472\n",
      "[472, 15]: MSE Loss: 0.0021\n",
      "[472, 30]: MSE Loss: 0.0009\n",
      "[472, 45]: MSE Loss: 0.0014\n",
      "[472, 60]: MSE Loss: 0.0013\n",
      "Epoch 473\n",
      "[473, 15]: MSE Loss: 0.0043\n",
      "[473, 30]: MSE Loss: 0.0019\n",
      "[473, 45]: MSE Loss: 0.0013\n",
      "[473, 60]: MSE Loss: 0.0014\n",
      "Epoch 474\n",
      "[474, 15]: MSE Loss: 0.0016\n",
      "[474, 30]: MSE Loss: 0.0014\n",
      "[474, 45]: MSE Loss: 0.0014\n",
      "[474, 60]: MSE Loss: 0.0013\n",
      "Epoch 475\n",
      "[475, 15]: MSE Loss: 0.0017\n",
      "[475, 30]: MSE Loss: 0.0016\n",
      "[475, 45]: MSE Loss: 0.0016\n",
      "[475, 60]: MSE Loss: 0.0015\n",
      "Epoch 476\n",
      "[476, 15]: MSE Loss: 0.0016\n",
      "[476, 30]: MSE Loss: 0.0011\n",
      "[476, 45]: MSE Loss: 0.0011\n",
      "[476, 60]: MSE Loss: 0.0009\n",
      "Epoch 477\n",
      "[477, 15]: MSE Loss: 0.0011\n",
      "[477, 30]: MSE Loss: 0.0021\n",
      "[477, 45]: MSE Loss: 0.0012\n",
      "[477, 60]: MSE Loss: 0.0014\n",
      "Epoch 478\n",
      "[478, 15]: MSE Loss: 0.0028\n",
      "[478, 30]: MSE Loss: 0.0016\n",
      "[478, 45]: MSE Loss: 0.0015\n",
      "[478, 60]: MSE Loss: 0.0010\n",
      "Epoch 479\n",
      "[479, 15]: MSE Loss: 0.0013\n",
      "[479, 30]: MSE Loss: 0.0012\n",
      "[479, 45]: MSE Loss: 0.0011\n",
      "[479, 60]: MSE Loss: 0.0010\n",
      "Epoch 480\n",
      "[480, 15]: MSE Loss: 0.0014\n",
      "[480, 30]: MSE Loss: 0.0016\n",
      "[480, 45]: MSE Loss: 0.0015\n",
      "[480, 60]: MSE Loss: 0.0013\n",
      "Test Loss: 0.04980\n",
      "########################################\n",
      "Epoch 481\n",
      "[481, 15]: MSE Loss: 0.0012\n",
      "[481, 30]: MSE Loss: 0.0008\n",
      "[481, 45]: MSE Loss: 0.0013\n",
      "[481, 60]: MSE Loss: 0.0056\n",
      "Epoch 482\n",
      "[482, 15]: MSE Loss: 0.0036\n",
      "[482, 30]: MSE Loss: 0.0022\n",
      "[482, 45]: MSE Loss: 0.0026\n",
      "[482, 60]: MSE Loss: 0.0017\n",
      "Epoch 483\n",
      "[483, 15]: MSE Loss: 0.0015\n",
      "[483, 30]: MSE Loss: 0.0015\n",
      "[483, 45]: MSE Loss: 0.0034\n",
      "[483, 60]: MSE Loss: 0.0015\n",
      "Epoch 484\n",
      "[484, 15]: MSE Loss: 0.0024\n",
      "[484, 30]: MSE Loss: 0.0016\n",
      "[484, 45]: MSE Loss: 0.0012\n",
      "[484, 60]: MSE Loss: 0.0014\n",
      "Epoch 485\n",
      "[485, 15]: MSE Loss: 0.0014\n",
      "[485, 30]: MSE Loss: 0.0027\n",
      "[485, 45]: MSE Loss: 0.0011\n",
      "[485, 60]: MSE Loss: 0.0012\n",
      "Epoch 486\n",
      "[486, 15]: MSE Loss: 0.0012\n",
      "[486, 30]: MSE Loss: 0.0010\n",
      "[486, 45]: MSE Loss: 0.0017\n",
      "[486, 60]: MSE Loss: 0.0009\n",
      "Epoch 487\n",
      "[487, 15]: MSE Loss: 0.0010\n",
      "[487, 30]: MSE Loss: 0.0020\n",
      "[487, 45]: MSE Loss: 0.0013\n",
      "[487, 60]: MSE Loss: 0.0014\n",
      "Epoch 488\n",
      "[488, 15]: MSE Loss: 0.0019\n",
      "[488, 30]: MSE Loss: 0.0011\n",
      "[488, 45]: MSE Loss: 0.0009\n",
      "[488, 60]: MSE Loss: 0.0015\n",
      "Epoch 489\n",
      "[489, 15]: MSE Loss: 0.0012\n",
      "[489, 30]: MSE Loss: 0.0011\n",
      "[489, 45]: MSE Loss: 0.0020\n",
      "[489, 60]: MSE Loss: 0.0010\n",
      "Epoch 490\n",
      "[490, 15]: MSE Loss: 0.0019\n",
      "[490, 30]: MSE Loss: 0.0017\n",
      "[490, 45]: MSE Loss: 0.0013\n",
      "[490, 60]: MSE Loss: 0.0017\n",
      "Test Loss: 0.15536\n",
      "########################################\n",
      "Epoch 491\n",
      "[491, 15]: MSE Loss: 0.0017\n",
      "[491, 30]: MSE Loss: 0.0042\n",
      "[491, 45]: MSE Loss: 0.0016\n",
      "[491, 60]: MSE Loss: 0.0020\n",
      "Epoch 492\n",
      "[492, 15]: MSE Loss: 0.0030\n",
      "[492, 30]: MSE Loss: 0.0018\n",
      "[492, 45]: MSE Loss: 0.0028\n",
      "[492, 60]: MSE Loss: 0.0020\n",
      "Epoch 493\n",
      "[493, 15]: MSE Loss: 0.0010\n",
      "[493, 30]: MSE Loss: 0.0027\n",
      "[493, 45]: MSE Loss: 0.0021\n",
      "[493, 60]: MSE Loss: 0.0017\n",
      "Epoch 494\n",
      "[494, 15]: MSE Loss: 0.0044\n",
      "[494, 30]: MSE Loss: 0.0012\n",
      "[494, 45]: MSE Loss: 0.0020\n",
      "[494, 60]: MSE Loss: 0.0010\n",
      "Epoch 495\n",
      "[495, 15]: MSE Loss: 0.0008\n",
      "[495, 30]: MSE Loss: 0.0018\n",
      "[495, 45]: MSE Loss: 0.0025\n",
      "[495, 60]: MSE Loss: 0.0018\n",
      "Epoch 496\n",
      "[496, 15]: MSE Loss: 0.0023\n",
      "[496, 30]: MSE Loss: 0.0043\n",
      "[496, 45]: MSE Loss: 0.0010\n",
      "[496, 60]: MSE Loss: 0.0022\n",
      "Epoch 497\n",
      "[497, 15]: MSE Loss: 0.0019\n",
      "[497, 30]: MSE Loss: 0.0021\n",
      "[497, 45]: MSE Loss: 0.0018\n",
      "[497, 60]: MSE Loss: 0.0021\n",
      "Epoch 498\n",
      "[498, 15]: MSE Loss: 0.0036\n",
      "[498, 30]: MSE Loss: 0.0053\n",
      "[498, 45]: MSE Loss: 0.0028\n",
      "[498, 60]: MSE Loss: 0.0048\n",
      "Epoch 499\n",
      "[499, 15]: MSE Loss: 0.0041\n",
      "[499, 30]: MSE Loss: 0.0021\n",
      "[499, 45]: MSE Loss: 0.0016\n",
      "[499, 60]: MSE Loss: 0.0023\n",
      "Epoch 500\n",
      "[500, 15]: MSE Loss: 0.0037\n",
      "[500, 30]: MSE Loss: 0.0023\n",
      "[500, 45]: MSE Loss: 0.0029\n",
      "[500, 60]: MSE Loss: 0.0015\n",
      "Test Loss: 0.05305\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding=1,\n",
    "                               stride=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding=1,\n",
    "                               stride=1)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding=1,\n",
    "                               stride=1)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=64,\n",
    "                               out_channels=128,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding=1,\n",
    "                               stride=1)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.linear1 = nn.Linear(7*7*128, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.linear2 = nn.Linear(64, 6)\n",
    "        self.linear3 = nn.Linear(8, 1) # 6 + 2 roi_info features\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "    def forward(self, img, roi_info):\n",
    "        o1 = F.relu(self.conv1(img))\n",
    "        o1 = self.pool(self.bn1(o1))\n",
    "        #print(o1.shape)\n",
    "        \n",
    "        o2 = F.relu(self.conv2(o1))\n",
    "        o2 = self.pool(self.bn2(o2))\n",
    "        #print(o2.shape)\n",
    "        \n",
    "        o3 = F.relu(self.conv3(o2))\n",
    "        o3 = self.pool(self.bn3(o3))\n",
    "        #print(o3.shape)\n",
    "        \n",
    "        o4 = F.relu(self.conv4(o3))\n",
    "        o4 = self.pool(self.bn4(o4))\n",
    "        #print(o4.shape)\n",
    "\n",
    "        # Keep batch dim and flatten\n",
    "        conv_out = o4.view(-1, 7*7*128)\n",
    "        \n",
    "        l1 = F.relu(self.linear1(conv_out))\n",
    "        l1 = self.bn5(l1)\n",
    "        \n",
    "        l2 = F.relu(self.linear2(l1))\n",
    "        \n",
    "        # Concat roi_info along with the processed conv features\n",
    "        concat = torch.cat((l2, roi_info), 1)\n",
    "        #print(l1[0])\n",
    "\n",
    "        # Pay attention to the activation here\n",
    "        out = self.linear3(concat)\n",
    "        \n",
    "        return out\n",
    "\n",
    "INTERVAL = 15\n",
    "    \n",
    "model = ConvNet()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i_batch, batch in enumerate(train_loader):\n",
    "        (x_img, x_roi_info), y_volume = batch[0], batch[1].cuda()\n",
    "        x_img = x_img.cuda()\n",
    "        x_roi_info = x_roi_info.cuda()\n",
    "        \n",
    "        assert len(x_img) == len(y_volume)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_y_volume = model(x_img, x_roi_info)\n",
    "        \n",
    "        loss = criterion(pred_y_volume, y_volume)\n",
    "        #print(pred_y_volume.shape, y_volume.shape)\n",
    "        #print(pred_y_volume[0], y_volume[0])\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % INTERVAL == INTERVAL-1:\n",
    "            running_loss += running_loss\n",
    "            print(f\"[{epoch+1}, {i_batch+1}]: MSE Loss: {running_loss/INTERVAL:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    if epoch % 10 == 9:\n",
    "        test_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in test_loader:\n",
    "                (x_img, x_roi_info), y_volume = batch[0], batch[1].cuda()\n",
    "                x_img = x_img.cuda()\n",
    "                x_roi_info = x_roi_info.cuda()\n",
    "                \n",
    "                loss = criterion(pred_y_volume, y_volume)\n",
    "                test_loss += loss.item()\n",
    "                total_samples += 1\n",
    "                \n",
    "            test_loss = test_loss / total_samples\n",
    "            \n",
    "            print(f\"Test Loss: {test_loss:.5f}\")\n",
    "            print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-58dd8f75e5c1>:35: RuntimeWarning: invalid value encountered in true_divide\n",
      "  filter1_8u = ((filter1 / filter1.max()) * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldnt find roi\n",
      "Couldnt find roi\n",
      "Couldnt find roi\n",
      "Couldnt find roi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1417.132227548531"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getModelPrediction(model, depth_frames):\n",
    "    roi = findBestFrameROI(depth_frames)\n",
    "    \n",
    "    if roi is not None:\n",
    "        h, w = roi.shape\n",
    "        h = h / IMG_HEIGHT\n",
    "        w = w / IMG_WIDTH\n",
    "        \n",
    "        # Normalize and resize roi\n",
    "        roi = np.divide(roi, MAX_VAL)\n",
    "        roi = cv2.resize(roi, NET_SIZE)\n",
    "        \n",
    "        # Convert everything to tensors\n",
    "        tensor_roi = torch.Tensor(roi).unsqueeze(0)\n",
    "        tensor_roi = tensor_roi.unsqueeze(0).cuda()\n",
    "        tensor_roi_info = torch.Tensor([h, w]).unsqueeze(0).cuda()\n",
    "\n",
    "        # Feed to model and get prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model(tensor_roi, tensor_roi_info)\n",
    "            return pred.item() * 4000 #* ANNOTATION_STD) + ANNOTATION_MEAN\n",
    "    else:\n",
    "        print(\"Couldnt find roi\")\n",
    "        return None\n",
    "        \n",
    "model.eval()\n",
    "preds = []\n",
    "for i in range(60):\n",
    "    i_str = str(i)\n",
    "    _str = '0' * (4 - len(i_str)) + i_str\n",
    "    sample_frames = natsorted(glob.glob(f'Dataset/12/depth/{_str}/c3/*.png'))\n",
    "\n",
    "    pred = getModelPrediction(model, sample_frames)\n",
    "    if pred is not None:\n",
    "        preds.append(pred)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520.0, 185.0, 202.0, 296.0, 363.0, 128.0, 3209.397, 1239.84, 471.6]\n",
      "734.981888888889 929.7103190574344\n",
      "[-0.23123535 -0.59156264 -0.57327737 -0.47217061 -0.40010515 -0.65287206\n",
      "  2.66149042  0.54302733 -0.28329457]\n"
     ]
    }
   ],
   "source": [
    "# The capacity of the containers in mL\n",
    "ANNOTATION = {1: 520.0, # Red cup\n",
    "              2: 185.0, # Small white cup\n",
    "              3: 202.0, # Small transparent cup\n",
    "              4: 296.0, # Green glass\n",
    "              5: 363.0, # Wine glass\n",
    "              6: 128.0, # Champagne flute\n",
    "              7: 3209.397, # Cereal box\n",
    "              8: 1239.84, # Biscuits box\n",
    "              9: 471.6} # Tea box\n",
    "\n",
    "vals = list(ANNOTATION.values())\n",
    "print(vals)\n",
    "\n",
    "mean = np.mean(vals)\n",
    "std = np.std(vals)\n",
    "print(mean, std)\n",
    "\n",
    "normalized = (vals - mean) / std\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def generatesubmission(feature,data_list):\n",
    "    df = pd.read_csv('submissions/submissionfile.csv',index_col=0)\n",
    "    df[feature] = data_list\n",
    "    df.to_csv('./submissions/submissionfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 453.84929329156876, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 467.14714401728145, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531, 1417.132227548531]\n"
     ]
    }
   ],
   "source": [
    "Container_Capacity_dic = {10:453.84929329156876,11:467.14714401728145,12:1417.132227548531}\n",
    "data_list=[]\n",
    "df = pd.read_csv('submissions/submissionfile.csv')\n",
    "for j in range(10,13):\n",
    "    for i in range((df['Container ID']==j).sum()):\n",
    "        data_list.append(Container_Capacity_dic[j])\n",
    "len(data_list)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatesubmission('Container Capacity',data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[1871.9310760498047, 1284.316062927246, 1461.5625143051147, 1879.4474601745605, 1661.1031293869019, 856.5754890441895, 1017.8735256195068, 1748.9001750946045, 1391.5421962738037, 1616.925597190857, 2383.138656616211, 1197.2930431365967, 1147.6399898529053, 1003.0750036239624, -1, 1350.6317138671875, 1389.357566833496, 1069.688081741333, 2054.701805114746, 1056.5693378448486, 2179.6462535858154, 1559.289574623108, 855.7561039924622, -1, 1801.5096187591553, 1219.1458940505981, 2135.6592178344727, 988.2728457450867, 1192.911148071289, 787.0398759841919, 1180.8658838272095, 1690.1371479034424, 1123.9075660705566, 1801.6518354415894, 1684.5309734344482, 1046.358585357666, 1222.6459980010986, 789.4732356071472, 1922.737717628479, 2352.247714996338, 1021.5753316879272, 1737.982988357544, 1548.818826675415, 1177.6173114776611, 1697.1685886383057, -1, 1578.3904790878296, 1542.87850856781, -1, 1863.6950254440308, 1114.0027046203613, 1112.0328903198242, 1040.4130220413208, 1299.3124723434448, 710.6072902679443, 819.8226690292358, 1698.6591815948486, 1522.2299098968506, 1595.4781770706177, 1302.6597499847412]\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[314.41840529441833,\n",
       " 184.81141328811646,\n",
       " 370.24587392807007,\n",
       " 508.09550285339355,\n",
       " 307.7709972858429,\n",
       " 385.80507040023804,\n",
       " 454.88622784614563,\n",
       " 332.1031928062439,\n",
       " 226.83916985988617,\n",
       " -1,\n",
       " 310.99146604537964,\n",
       " 236.20569705963135,\n",
       " 792.6722168922424,\n",
       " 367.5205111503601,\n",
       " 170.29240727424622,\n",
       " 465.75915813446045,\n",
       " 381.3762068748474,\n",
       " 562.5863671302795,\n",
       " 350.2958118915558,\n",
       " 404.23738956451416,\n",
       " 296.13450169563293,\n",
       " 238.06118965148926,\n",
       " 820.4612731933594,\n",
       " 438.13449144363403,\n",
       " 375.08848309516907,\n",
       " 161.4813208580017,\n",
       " 992.7847385406494,\n",
       " 469.58357095718384,\n",
       " 450.14268159866333,\n",
       " 545.3451871871948,\n",
       " 291.20680689811707,\n",
       " 453.5031318664551,\n",
       " 493.6419129371643,\n",
       " 620.7624077796936,\n",
       " 414.500892162323,\n",
       " 276.5718698501587,\n",
       " 551.2747764587402,\n",
       " 507.04532861709595,\n",
       " 282.9831838607788,\n",
       " 310.60516834259033,\n",
       " 512.2324228286743,\n",
       " 734.8564267158508,\n",
       " 376.46883726119995,\n",
       " 348.9348888397217,\n",
       " 263.36294412612915,\n",
       " 445.7263946533203,\n",
       " 694.770097732544,\n",
       " 754.7130584716797,\n",
       " 481.89595341682434,\n",
       " 876.6219615936279,\n",
       " 308.46211314201355,\n",
       " 322.5908875465393,\n",
       " 377.88665294647217,\n",
       " 582.7159881591797,\n",
       " 1110.1888418197632,\n",
       " 397.23092317581177,\n",
       " 309.3339204788208,\n",
       " 396.1363434791565,\n",
       " 458.554208278656,\n",
       " 131.74694776535034,\n",
       " 122.88028001785278,\n",
       " 251.1906921863556,\n",
       " 316.1046504974365,\n",
       " 168.4761941432953,\n",
       " 427.9462695121765,\n",
       " 373.89540672302246,\n",
       " 469.7617292404175,\n",
       " 383.45885276794434,\n",
       " -1,\n",
       " 434.3227744102478,\n",
       " 478.89935970306396,\n",
       " 265.743225812912,\n",
       " 723.6596345901489,\n",
       " 465.43824672698975,\n",
       " 227.36290097236633,\n",
       " 447.7308392524719,\n",
       " 364.0601336956024,\n",
       " 1574.8366117477417,\n",
       " 541.1635637283325,\n",
       " 583.6501717567444,\n",
       " 404.23527359962463,\n",
       " 207.9068124294281,\n",
       " 1108.848214149475,\n",
       " 873.4143972396851,\n",
       " 919.3561673164368,\n",
       " 462.68242597579956,\n",
       " 519.2768573760986,\n",
       " 552.6371598243713,\n",
       " -1,\n",
       " 373.66434931755066,\n",
       " 251.04865431785583,\n",
       " 1657.6541662216187,\n",
       " 391.3148045539856,\n",
       " 294.3001389503479,\n",
       " 185.67286431789398,\n",
       " 444.46396827697754,\n",
       " 410.2562665939331,\n",
       " -1,\n",
       " 404.40666675567627,\n",
       " 334.2384696006775,\n",
       " -1,\n",
       " 373.5070824623108,\n",
       " 864.6527528762817,\n",
       " 383.17546248435974,\n",
       " 998.87615442276,\n",
       " 593.7591195106506,\n",
       " 224.00054335594177,\n",
       " 357.73804783821106,\n",
       " 427.8702735900879,\n",
       " 302.679568529129,\n",
       " 526.2293815612793,\n",
       " 584.814727306366,\n",
       " 544.343888759613,\n",
       " 532.501220703125,\n",
       " 356.8055331707001,\n",
       " 900.6273150444031,\n",
       " 271.3961601257324,\n",
       " 204.80823516845703,\n",
       " -1,\n",
       " 858.1774234771729,\n",
       " 338.9003872871399,\n",
       " 167.1360433101654,\n",
       " 394.72323656082153,\n",
       " 196.11190259456635,\n",
       " 142.7227258682251,\n",
       " 330.6809067726135,\n",
       " 632.054328918457,\n",
       " 453.01344990730286,\n",
       " 397.76289463043213,\n",
       " 354.3078303337097,\n",
       " 409.806489944458,\n",
       " 506.24382495880127,\n",
       " 157.05561637878418,\n",
       " 450.7042467594147,\n",
       " 458.89580249786377,\n",
       " 426.4054298400879,\n",
       " 269.2393660545349,\n",
       " -1,\n",
       " 260.6362998485565,\n",
       " 232.02165961265564,\n",
       " 505.4357051849365,\n",
       " 489.8967444896698,\n",
       " 378.3983588218689,\n",
       " 510.2895498275757,\n",
       " 713.6094570159912,\n",
       " -1,\n",
       " 331.7519426345825,\n",
       " 457.96525478363037,\n",
       " 629.4891834259033,\n",
       " 341.08567237854004,\n",
       " 680.0262928009033,\n",
       " 709.9543213844299,\n",
       " 1015.6095027923584,\n",
       " 593.564510345459,\n",
       " 1046.5017557144165,\n",
       " -1,\n",
       " 353.8975119590759,\n",
       " 306.6856861114502,\n",
       " 282.7589511871338,\n",
       " 372.4732995033264,\n",
       " 347.1727967262268,\n",
       " 327.35341787338257,\n",
       " 367.0366406440735,\n",
       " 388.05490732192993,\n",
       " 433.87407064437866,\n",
       " 743.3810830116272,\n",
       " 284.4359278678894,\n",
       " 274.2692530155182,\n",
       " 1871.9310760498047,\n",
       " 1284.316062927246,\n",
       " 1461.5625143051147,\n",
       " 1879.4474601745605,\n",
       " 1661.1031293869019,\n",
       " 856.5754890441895,\n",
       " 1017.8735256195068,\n",
       " 1748.9001750946045,\n",
       " 1391.5421962738037,\n",
       " 1616.925597190857,\n",
       " 2383.138656616211,\n",
       " 1197.2930431365967,\n",
       " 1147.6399898529053,\n",
       " 1003.0750036239624,\n",
       " -1,\n",
       " 1350.6317138671875,\n",
       " 1389.357566833496,\n",
       " 1069.688081741333,\n",
       " 2054.701805114746,\n",
       " 1056.5693378448486,\n",
       " 2179.6462535858154,\n",
       " 1559.289574623108,\n",
       " 855.7561039924622,\n",
       " -1,\n",
       " 1801.5096187591553,\n",
       " 1219.1458940505981,\n",
       " 2135.6592178344727,\n",
       " 988.2728457450867,\n",
       " 1192.911148071289,\n",
       " 787.0398759841919,\n",
       " 1180.8658838272095,\n",
       " 1690.1371479034424,\n",
       " 1123.9075660705566,\n",
       " 1801.6518354415894,\n",
       " 1684.5309734344482,\n",
       " 1046.358585357666,\n",
       " 1222.6459980010986,\n",
       " 789.4732356071472,\n",
       " 1922.737717628479,\n",
       " 2352.247714996338,\n",
       " 1021.5753316879272,\n",
       " 1737.982988357544,\n",
       " 1548.818826675415,\n",
       " 1177.6173114776611,\n",
       " 1697.1685886383057,\n",
       " -1,\n",
       " 1578.3904790878296,\n",
       " 1542.87850856781,\n",
       " -1,\n",
       " 1863.6950254440308,\n",
       " 1114.0027046203613,\n",
       " 1112.0328903198242,\n",
       " 1040.4130220413208,\n",
       " 1299.3124723434448,\n",
       " 710.6072902679443,\n",
       " 819.8226690292358,\n",
       " 1698.6591815948486,\n",
       " 1522.2299098968506,\n",
       " 1595.4781770706177,\n",
       " 1302.6597499847412]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list+=preds\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
